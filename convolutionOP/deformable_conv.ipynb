{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeformConv2d(nn.Module):\n",
    "    def __init__(self, inc, outc, kernel_size=3, padding=1, stride=1, bias=None, modulation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            modulation (bool, optional): If True, Modulated Defomable Convolution (Deformable ConvNets v2).\n",
    "        \"\"\"\n",
    "        super(DeformConv2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.zero_padding = nn.ZeroPad2d(padding)\n",
    "        # conv则是实际进行的卷积操作，注意这里步长设置为卷积核大小，\n",
    "        # 因为与该卷积核进行卷积操作的特征图是由输出特征图中每个点扩展为其对应卷积核那么多个点后生成的。\n",
    "        self.conv = nn.Conv2d(inc, outc, kernel_size=kernel_size, stride=kernel_size, bias=bias)\n",
    "        # p_conv是生成offsets所使用的卷积，输出通道数为卷积核尺寸的平方的2倍，\n",
    "        # 代表对应卷积核每个位置横纵坐标都有偏移量。\n",
    "        self.p_conv = nn.Conv2d(inc, 2*kernel_size*kernel_size, kernel_size=3, padding=1, stride=stride)\n",
    "        nn.init.constant_(self.p_conv.weight, 0)\n",
    "        self.p_conv.register_backward_hook(self._set_lr)\n",
    " \n",
    "        self.modulation = modulation # modulation是可选参数,若设置为True,那么在进行卷积操作时,对应卷积核的每个位置都会分配一个权重。\n",
    "        if modulation:\n",
    "            self.m_conv = nn.Conv2d(inc, kernel_size*kernel_size, kernel_size=3, padding=1, stride=stride)\n",
    "            nn.init.constant_(self.m_conv.weight, 0)\n",
    "            self.m_conv.register_backward_hook(self._set_lr)\n",
    " \n",
    "    @staticmethod\n",
    "    def _set_lr(module, grad_input, grad_output):\n",
    "        grad_input = (grad_input[i] * 0.1 for i in range(len(grad_input)))\n",
    "        grad_output = (grad_output[i] * 0.1 for i in range(len(grad_output)))\n",
    " \n",
    "    def forward(self, x):\n",
    "        offset = self.p_conv(x)\n",
    "        if self.modulation:\n",
    "            m = torch.sigmoid(self.m_conv(x))\n",
    " \n",
    "        dtype = offset.data.type()\n",
    "        ks = self.kernel_size\n",
    "        N = offset.size(1) // 2\n",
    " \n",
    "        if self.padding:\n",
    "            x = self.zero_padding(x)\n",
    " \n",
    "        # (b, 2N, h, w)\n",
    "        p = self._get_p(offset, dtype)\n",
    " \n",
    "        # (b, h, w, 2N)\n",
    "        p = p.contiguous().permute(0, 2, 3, 1)\n",
    "        q_lt = p.detach().floor()\n",
    "        q_rb = q_lt + 1\n",
    " \n",
    "        q_lt = torch.cat([torch.clamp(q_lt[..., :N], 0, x.size(2)-1), torch.clamp(q_lt[..., N:], 0, x.size(3)-1)], dim=-1).long()\n",
    "        q_rb = torch.cat([torch.clamp(q_rb[..., :N], 0, x.size(2)-1), torch.clamp(q_rb[..., N:], 0, x.size(3)-1)], dim=-1).long()\n",
    "        q_lb = torch.cat([q_lt[..., :N], q_rb[..., N:]], dim=-1)\n",
    "        q_rt = torch.cat([q_rb[..., :N], q_lt[..., N:]], dim=-1)\n",
    " \n",
    "        # clip p\n",
    "        p = torch.cat([torch.clamp(p[..., :N], 0, x.size(2)-1), torch.clamp(p[..., N:], 0, x.size(3)-1)], dim=-1)\n",
    " \n",
    "        # bilinear kernel (b, h, w, N)\n",
    "        g_lt = (1 + (q_lt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_lt[..., N:].type_as(p) - p[..., N:]))\n",
    "        g_rb = (1 - (q_rb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_rb[..., N:].type_as(p) - p[..., N:]))\n",
    "        g_lb = (1 + (q_lb[..., :N].type_as(p) - p[..., :N])) * (1 - (q_lb[..., N:].type_as(p) - p[..., N:]))\n",
    "        g_rt = (1 - (q_rt[..., :N].type_as(p) - p[..., :N])) * (1 + (q_rt[..., N:].type_as(p) - p[..., N:]))\n",
    " \n",
    "        # (b, c, h, w, N)\n",
    "        x_q_lt = self._get_x_q(x, q_lt, N)\n",
    "        x_q_rb = self._get_x_q(x, q_rb, N)\n",
    "        x_q_lb = self._get_x_q(x, q_lb, N)\n",
    "        x_q_rt = self._get_x_q(x, q_rt, N)\n",
    " \n",
    "        # (b, c, h, w, N)\n",
    "        x_offset = g_lt.unsqueeze(dim=1) * x_q_lt + \\\n",
    "                   g_rb.unsqueeze(dim=1) * x_q_rb + \\\n",
    "                   g_lb.unsqueeze(dim=1) * x_q_lb + \\\n",
    "                   g_rt.unsqueeze(dim=1) * x_q_rt\n",
    " \n",
    "        # modulation\n",
    "        if self.modulation:\n",
    "            m = m.contiguous().permute(0, 2, 3, 1)\n",
    "            m = m.unsqueeze(dim=1)\n",
    "            m = torch.cat([m for _ in range(x_offset.size(1))], dim=1)\n",
    "            x_offset *= m\n",
    " \n",
    "        x_offset = self._reshape_x_offset(x_offset, ks)\n",
    "        out = self.conv(x_offset)\n",
    " \n",
    "        return out\n",
    " \n",
    "    def _get_p_n(self, N, dtype):\n",
    "        # 由于卷积核中心点位置是其尺寸的一半，于是中心点向左（上）方向移动尺寸的一半就得到起始点，向右（下）方向移动另一半就得到终止点\n",
    "        p_n_x, p_n_y = torch.meshgrid(\n",
    "            torch.arange(-(self.kernel_size-1)//2, (self.kernel_size-1)//2+1),\n",
    "            torch.arange(-(self.kernel_size-1)//2, (self.kernel_size-1)//2+1))\n",
    "        # (2N, 1)\n",
    "        p_n = torch.cat([torch.flatten(p_n_x), torch.flatten(p_n_y)], 0)\n",
    "        p_n = p_n.view(1, 2*N, 1, 1).type(dtype)\n",
    " \n",
    "        return p_n\n",
    " \n",
    "    def _get_p_0(self, h, w, N, dtype):\n",
    "        # p0_y、p0_x就是输出特征图每点映射到输入特征图上的纵、横坐标值。\n",
    "        p_0_x, p_0_y = torch.meshgrid(\n",
    "            torch.arange(1, h*self.stride+1, self.stride),\n",
    "            torch.arange(1, w*self.stride+1, self.stride))\n",
    "        \n",
    "        p_0_x = torch.flatten(p_0_x).view(1, 1, h, w).repeat(1, N, 1, 1)\n",
    "        p_0_y = torch.flatten(p_0_y).view(1, 1, h, w).repeat(1, N, 1, 1)\n",
    "        p_0 = torch.cat([p_0_x, p_0_y], 1).type(dtype)\n",
    " \n",
    "        return p_0\n",
    "    \n",
    "    # 输出特征图上每点（对应卷积核中心）加上其对应卷积核每个位置的相对（横、纵）坐标后再加上自学习的（横、纵坐标）偏移量。\n",
    "    # p0就是将输出特征图每点对应到卷积核中心，然后映射到输入特征图中的位置；\n",
    "    # pn则是p0对应卷积核每个位置的相对坐标；\n",
    "    def _get_p(self, offset, dtype):\n",
    "        N, h, w = offset.size(1)//2, offset.size(2), offset.size(3)\n",
    " \n",
    "        # (1, 2N, 1, 1)\n",
    "        p_n = self._get_p_n(N, dtype)\n",
    "        # (1, 2N, h, w)\n",
    "        p_0 = self._get_p_0(h, w, N, dtype)\n",
    "        p = p_0 + p_n + offset\n",
    "        return p\n",
    " \n",
    "    def _get_x_q(self, x, q, N):\n",
    "        # 计算双线性插值点的4邻域点对应的权重\n",
    "        b, h, w, _ = q.size()\n",
    "        padded_w = x.size(3)\n",
    "        c = x.size(1)\n",
    "        # (b, c, h*w)\n",
    "        x = x.contiguous().view(b, c, -1)\n",
    " \n",
    "        # (b, h, w, N)\n",
    "        index = q[..., :N]*padded_w + q[..., N:]  # offset_x*w + offset_y\n",
    "        # (b, c, h*w*N)\n",
    "        index = index.contiguous().unsqueeze(dim=1).expand(-1, c, -1, -1, -1).contiguous().view(b, c, -1)\n",
    " \n",
    "        x_offset = x.gather(dim=-1, index=index).contiguous().view(b, c, h, w, N)\n",
    " \n",
    "        return x_offset\n",
    " \n",
    "    @staticmethod\n",
    "    def _reshape_x_offset(x_offset, ks):\n",
    "        b, c, h, w, N = x_offset.size()\n",
    "        x_offset = torch.cat([x_offset[..., s:s+ks].contiguous().view(b, c, h, w*ks) for s in range(0, N, ks)], dim=-1)\n",
    "        x_offset = x_offset.contiguous().view(b, c, h*ks, w*ks)\n",
    " \n",
    "        return x_offset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
