Detected 2 CUDA Capable device(s)

Device 0: "A100-SXM-80GB"
  CUDA Driver Version / Runtime Version          11.0 / 11.0
  CUDA Capability Major/Minor version number:    8.0
[MemUsageChange] Init CUDA: CPU +333, GPU +0, now: CPU 340, GPU 2327 (MiB)
[MemUsageChange] Init builder kernel library: CPU +418, GPU +132, now: CPU 775, GPU 2459 (MiB)
----------------------------------------------------------------
Input filename:   LeNet5.onnx
ONNX IR version:  0.0.6
Opset version:    11
Producer name:    pytorch
Producer version: 1.7
Domain:           
Model version:    0
Doc string:       
----------------------------------------------------------------
Registered plugin creator - ::GridAnchor_TRT version 1
Registered plugin creator - ::GridAnchorRect_TRT version 1
Registered plugin creator - ::NMS_TRT version 1
Registered plugin creator - ::Reorg_TRT version 1
Registered plugin creator - ::Region_TRT version 1
Registered plugin creator - ::Clip_TRT version 1
Registered plugin creator - ::LReLU_TRT version 1
Registered plugin creator - ::PriorBox_TRT version 1
Registered plugin creator - ::Normalize_TRT version 1
Registered plugin creator - ::ScatterND version 1
Registered plugin creator - ::RPROI_TRT version 1
Registered plugin creator - ::BatchedNMS_TRT version 1
Registered plugin creator - ::BatchedNMSDynamic_TRT version 1
Registered plugin creator - ::BatchTilePlugin_TRT version 1
Registered plugin creator - ::FlattenConcat_TRT version 1
Registered plugin creator - ::CropAndResize version 1
Registered plugin creator - ::CropAndResizeDynamic version 1
Registered plugin creator - ::DetectionLayer_TRT version 1
Registered plugin creator - ::EfficientNMS_TRT version 1
Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1
Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1
Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1
Registered plugin creator - ::ProposalDynamic version 1
Registered plugin creator - ::Proposal version 1
Registered plugin creator - ::ProposalLayer_TRT version 1
Registered plugin creator - ::PyramidROIAlign_TRT version 1
Registered plugin creator - ::ResizeNearest_TRT version 1
Registered plugin creator - ::Split version 1
Registered plugin creator - ::SpecialSlice_TRT version 1
Registered plugin creator - ::InstanceNormalization_TRT version 1
Registered plugin creator - ::InstanceNormalization_TRT version 2
Registered plugin creator - ::CoordConvAC version 1
Registered plugin creator - ::DecodeBbox3DPlugin version 1
Registered plugin creator - ::GenerateDetection_TRT version 1
Registered plugin creator - ::MultilevelCropAndResize_TRT version 1
Registered plugin creator - ::MultilevelProposeROI_TRT version 1
Registered plugin creator - ::NMSDynamic_TRT version 1
Registered plugin creator - ::PillarScatterPlugin version 1
Registered plugin creator - ::VoxelGeneratorPlugin version 1
Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1
Adding network input: input with dtype: float32, dimensions: (-1, 1, 32, 32)
Registering tensor: input for ONNX tensor: input
Importing initializer: conv1.bias
Importing initializer: conv1.weight
Importing initializer: conv2.bias
Importing initializer: conv2.weight
Importing initializer: fc1.bias
Importing initializer: fc1.weight
Importing initializer: fc2.bias
Importing initializer: fc2.weight
Importing initializer: fc3.bias
Importing initializer: fc3.weight
Parsing node: Conv_0 [Conv]
Searching for input: input
Searching for input: conv1.weight
Searching for input: conv1.bias
Conv_0 [Conv] inputs: [input -> (-1, 1, 32, 32)[FLOAT]], [conv1.weight -> (6, 1, 5, 5)[FLOAT]], [conv1.bias -> (6)[FLOAT]], 
Convolution input dimensions: (-1, 1, 32, 32)
Registering layer: Conv_0 for ONNX node: Conv_0
Using kernel: (5, 5), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 6
Convolution output dimensions: (-1, 6, 28, 28)
Registering tensor: 11 for ONNX tensor: 11
Conv_0 [Conv] outputs: [11 -> (-1, 6, 28, 28)[FLOAT]], 
Parsing node: Relu_1 [Relu]
Searching for input: 11
Relu_1 [Relu] inputs: [11 -> (-1, 6, 28, 28)[FLOAT]], 
Registering layer: Relu_1 for ONNX node: Relu_1
Registering tensor: 12 for ONNX tensor: 12
Relu_1 [Relu] outputs: [12 -> (-1, 6, 28, 28)[FLOAT]], 
Parsing node: MaxPool_2 [MaxPool]
Searching for input: 12
MaxPool_2 [MaxPool] inputs: [12 -> (-1, 6, 28, 28)[FLOAT]], 
Registering layer: MaxPool_2 for ONNX node: MaxPool_2
Registering tensor: 13 for ONNX tensor: 13
MaxPool_2 [MaxPool] outputs: [13 -> (-1, 6, 14, 14)[FLOAT]], 
Parsing node: Conv_3 [Conv]
Searching for input: 13
Searching for input: conv2.weight
Searching for input: conv2.bias
Conv_3 [Conv] inputs: [13 -> (-1, 6, 14, 14)[FLOAT]], [conv2.weight -> (16, 6, 5, 5)[FLOAT]], [conv2.bias -> (16)[FLOAT]], 
Convolution input dimensions: (-1, 6, 14, 14)
Registering layer: Conv_3 for ONNX node: Conv_3
Using kernel: (5, 5), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 16
Convolution output dimensions: (-1, 16, 10, 10)
Registering tensor: 14 for ONNX tensor: 14
Conv_3 [Conv] outputs: [14 -> (-1, 16, 10, 10)[FLOAT]], 
Parsing node: Relu_4 [Relu]
Searching for input: 14
Relu_4 [Relu] inputs: [14 -> (-1, 16, 10, 10)[FLOAT]], 
Registering layer: Relu_4 for ONNX node: Relu_4
Registering tensor: 15 for ONNX tensor: 15
Relu_4 [Relu] outputs: [15 -> (-1, 16, 10, 10)[FLOAT]], 
Parsing node: MaxPool_5 [MaxPool]
Searching for input: 15
MaxPool_5 [MaxPool] inputs: [15 -> (-1, 16, 10, 10)[FLOAT]], 
Registering layer: MaxPool_5 for ONNX node: MaxPool_5
Registering tensor: 16 for ONNX tensor: 16
MaxPool_5 [MaxPool] outputs: [16 -> (-1, 16, 5, 5)[FLOAT]], 
Parsing node: Flatten_6 [Flatten]
Searching for input: 16
Flatten_6 [Flatten] inputs: [16 -> (-1, 16, 5, 5)[FLOAT]], 
onnx2trt_utils.cpp:369: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
Registering layer: Flatten_6 for ONNX node: Flatten_6
Registering tensor: 17 for ONNX tensor: 17
Flatten_6 [Flatten] outputs: [17 -> (-1, 400)[FLOAT]], 
Parsing node: Gemm_7 [Gemm]
Searching for input: 17
Searching for input: fc1.weight
Searching for input: fc1.bias
Gemm_7 [Gemm] inputs: [17 -> (-1, 400)[FLOAT]], [fc1.weight -> (120, 400)[FLOAT]], [fc1.bias -> (120)[FLOAT]], 
Registering layer: fc1.weight for ONNX node: fc1.weight
Using opA: 0 opB: 1
Registering layer: Gemm_7 for ONNX node: Gemm_7
Registering layer: fc1.bias for ONNX node: fc1.bias
Registering tensor: 18 for ONNX tensor: 18
Gemm_7 [Gemm] outputs: [18 -> (-1, 120)[FLOAT]], 
Parsing node: Relu_8 [Relu]
Searching for input: 18
Relu_8 [Relu] inputs: [18 -> (-1, 120)[FLOAT]], 
Registering layer: Relu_8 for ONNX node: Relu_8
Registering tensor: 19 for ONNX tensor: 19
Relu_8 [Relu] outputs: [19 -> (-1, 120)[FLOAT]], 
Parsing node: Gemm_9 [Gemm]
Searching for input: 19
Searching for input: fc2.weight
Searching for input: fc2.bias
Gemm_9 [Gemm] inputs: [19 -> (-1, 120)[FLOAT]], [fc2.weight -> (84, 120)[FLOAT]], [fc2.bias -> (84)[FLOAT]], 
Registering layer: fc2.weight for ONNX node: fc2.weight
Using opA: 0 opB: 1
Registering layer: Gemm_9 for ONNX node: Gemm_9
Registering layer: fc2.bias for ONNX node: fc2.bias
Registering tensor: 20 for ONNX tensor: 20
Gemm_9 [Gemm] outputs: [20 -> (-1, 84)[FLOAT]], 
Parsing node: Relu_10 [Relu]
Searching for input: 20
Relu_10 [Relu] inputs: [20 -> (-1, 84)[FLOAT]], 
Registering layer: Relu_10 for ONNX node: Relu_10
Registering tensor: 21 for ONNX tensor: 21
Relu_10 [Relu] outputs: [21 -> (-1, 84)[FLOAT]], 
Parsing node: Gemm_11 [Gemm]
Searching for input: 21
Searching for input: fc3.weight
Searching for input: fc3.bias
Gemm_11 [Gemm] inputs: [21 -> (-1, 84)[FLOAT]], [fc3.weight -> (10, 84)[FLOAT]], [fc3.bias -> (10)[FLOAT]], 
Registering layer: fc3.weight for ONNX node: fc3.weight
Using opA: 0 opB: 1
Registering layer: Gemm_11 for ONNX node: Gemm_11
Registering layer: fc3.bias for ONNX node: fc3.bias
Registering tensor: output_1 for ONNX tensor: output
Gemm_11 [Gemm] outputs: [output -> (-1, 10)[FLOAT]], 
Marking output_1 as output: output
Applying generic optimizations to the graph for inference.
Original: 24 layers
After dead-layer removal: 24 layers
Running: ConstShuffleFusion on fc1.bias
ConstShuffleFusion: Fusing fc1.bias with (Unnamed Layer* 15) [Shuffle]
Running: ConstShuffleFusion on fc2.bias
ConstShuffleFusion: Fusing fc2.bias with (Unnamed Layer* 21) [Shuffle]
Running: ConstShuffleFusion on fc3.bias
ConstShuffleFusion: Fusing fc3.bias with (Unnamed Layer* 27) [Shuffle]
After Myelin optimization: 21 layers
Running: MatMulToConvTransform on Gemm_7
Convert layer type of Gemm_7 from MATRIX_MULTIPLY to CONVOLUTION
Running: MatMulToConvTransform on Gemm_9
Convert layer type of Gemm_9 from MATRIX_MULTIPLY to CONVOLUTION
Running: MatMulToConvTransform on Gemm_11
Convert layer type of Gemm_11 from MATRIX_MULTIPLY to CONVOLUTION
Running: ShuffleShuffleFusion on Flatten_6
ShuffleShuffleFusion: Fusing Flatten_6 with reshape_before_Gemm_7
Running: ConvReshapeBiasAddFusion on Gemm_7
Running: ConvReshapeBiasAddFusion on Gemm_9
Running: ConvReshapeBiasAddFusion on Gemm_11
Applying ScaleNodes fusions.
After scale fusion: 17 layers
Running: SqueezePushDownFork on reshape_after_Gemm_7
-----------SqueezePushDown kSQUEEZE_FORK case: Gemm_7 --> reshape_after_Gemm_7 --> Relu_8
Running: SqueezePushDownFork on reshape_after_Gemm_9
-----------SqueezePushDown kSQUEEZE_FORK case: Gemm_9 --> reshape_after_Gemm_9 --> Relu_10
Running: ShuffleShuffleFusion on squeeze_after_Relu_8
ShuffleShuffleFusion: Fusing squeeze_after_Relu_8 with reshape_before_Gemm_9
Running: ShuffleShuffleFusion on squeeze_after_Relu_10
ShuffleShuffleFusion: Fusing squeeze_after_Relu_10 with reshape_before_Gemm_11
Running: ConvReluFusion on Conv_0
ConvReluFusion: Fusing Conv_0 with Relu_1
Running: ConvReluFusion on Conv_3
ConvReluFusion: Fusing Conv_3 with Relu_4
Running: ConvReluFusion on Gemm_7
ConvReluFusion: Fusing Gemm_7 with Relu_8
Running: ShuffleErasure on squeeze_after_Relu_8 + reshape_before_Gemm_9
Removing squeeze_after_Relu_8 + reshape_before_Gemm_9
Running: ConvReluFusion on Gemm_9
ConvReluFusion: Fusing Gemm_9 with Relu_10
Running: ShuffleErasure on squeeze_after_Relu_10 + reshape_before_Gemm_11
Removing squeeze_after_Relu_10 + reshape_before_Gemm_11
After dupe layer removal: 9 layers
After final dead-layer removal: 9 layers
After tensor merging: 9 layers
After vertical fusions: 9 layers
After dupe layer removal: 9 layers
After final dead-layer removal: 9 layers
After tensor merging: 9 layers
After slice removal: 9 layers
After concat removal: 9 layers
Trying to split Reshape and strided tensor
Graph construction and optimization completed in 0.0850815 seconds.
Using cublasLt as a tactic source
[MemUsageChange] Init cuBLAS/cuBLASLt: CPU +618, GPU +256, now: CPU 1394, GPU 2715 (MiB)
Using cuDNN as a tactic source
[MemUsageChange] Init cuDNN: CPU +127, GPU +58, now: CPU 1521, GPU 2773 (MiB)
Local timing cache in use. Profiling results in this builder pass will not be stored.
Constructing optimization profile number 0 [1/1].
Reserving memory for host IO tensors. Host: 0 bytes
=============== Computing reformatting costs
*************** Autotuning Reformat: Float(1024,1024,32,1) -> Float(1024,1,32,1) ***************
--------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00633379
Tactic: 0x00000000000003ea Time: 0.0094877
Tactic: 0x0000000000000000 Time: 0.00663132
Fastest Tactic: 0x00000000000003e8 Time: 0.00633379
*************** Autotuning Reformat: Float(1024,1024,32,1) -> Float(1024,1:4,32,1) ***************
--------------- Timing Runner: Optimizer Reformat(input -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00667776
Tactic: 0x00000000000003ea Time: 0.00961341
Tactic: 0x0000000000000000 Time: 0.00668416
Fastest Tactic: 0x00000000000003e8 Time: 0.00667776
=============== Computing reformatting costs
*************** Autotuning Reformat: Float(4704,784,28,1) -> Float(1568,1:4,56,2) ***************
--------------- Timing Runner: Optimizer Reformat(12 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00676736
Tactic: 0x00000000000003ea Time: 0.00955855
Tactic: 0x0000000000000000 Time: 0.00684778
Fastest Tactic: 0x00000000000003e8 Time: 0.00676736
*************** Autotuning Reformat: Float(4704,1,168,6) -> Float(4704,784,28,1) ***************
--------------- Timing Runner: Optimizer Reformat(12 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00674261
Tactic: 0x00000000000003ea Time: 0.0147678
Tactic: 0x0000000000000000 Time: 0.00686106
Fastest Tactic: 0x00000000000003e8 Time: 0.00674261
*************** Autotuning Reformat: Float(4704,1,168,6) -> Float(1568,1:4,56,2) ***************
--------------- Timing Runner: Optimizer Reformat(12 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00682514
Tactic: 0x00000000000003ea Time: 0.0147659
Tactic: 0x0000000000000000 Time: 0.0068443
Fastest Tactic: 0x00000000000003e8 Time: 0.00682514
*************** Autotuning Reformat: Float(1568,1:4,56,2) -> Float(4704,784,28,1) ***************
--------------- Timing Runner: Optimizer Reformat(12 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00692876
Tactic: 0x00000000000003ea Time: 0.015184
Tactic: 0x0000000000000000 Time: 0.00695044
Fastest Tactic: 0x00000000000003e8 Time: 0.00692876
=============== Computing reformatting costs
*************** Autotuning Reformat: Float(1176,196,14,1) -> Float(1176,1,84,6) ***************
--------------- Timing Runner: Optimizer Reformat(13 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00664931
Tactic: 0x00000000000003ea Time: 0.0095811
Tactic: 0x0000000000000000 Time: 0.00688022
Fastest Tactic: 0x00000000000003e8 Time: 0.00664931
*************** Autotuning Reformat: Float(1176,196,14,1) -> Float(392,1:4,28,2) ***************
--------------- Timing Runner: Optimizer Reformat(13 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00670421
Tactic: 0x00000000000003ea Time: 0.00951881
Tactic: 0x0000000000000000 Time: 0.00690003
Fastest Tactic: 0x00000000000003e8 Time: 0.00670421
*************** Autotuning Reformat: Float(1176,1,84,6) -> Float(1176,196,14,1) ***************
--------------- Timing Runner: Optimizer Reformat(13 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00659012
Tactic: 0x00000000000003ea Time: 0.0144974
Tactic: 0x0000000000000000 Time: 0.00680043
Fastest Tactic: 0x00000000000003e8 Time: 0.00659012
*************** Autotuning Reformat: Float(1176,1,84,6) -> Float(392,1:4,28,2) ***************
--------------- Timing Runner: Optimizer Reformat(13 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00679659
Tactic: 0x00000000000003ea Time: 0.0144818
Tactic: 0x0000000000000000 Time: 0.00695684
Fastest Tactic: 0x00000000000003e8 Time: 0.00679659
*************** Autotuning Reformat: Float(392,1:4,28,2) -> Float(1176,196,14,1) ***************
--------------- Timing Runner: Optimizer Reformat(13 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00670848
Tactic: 0x00000000000003ea Time: 0.0150001
Tactic: 0x0000000000000000 Time: 0.0066056
Fastest Tactic: 0x0000000000000000 Time: 0.0066056
*************** Autotuning Reformat: Float(392,1:4,28,2) -> Float(1176,1,84,6) ***************
--------------- Timing Runner: Optimizer Reformat(13 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00666228
Tactic: 0x00000000000003ea Time: 0.0147399
Tactic: 0x0000000000000000 Time: 0.00677867
Fastest Tactic: 0x00000000000003e8 Time: 0.00666228
=============== Computing reformatting costs
*************** Autotuning Reformat: Float(1600,100,10,1) -> Float(400,1:4,40,4) ***************
--------------- Timing Runner: Optimizer Reformat(15 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00663948
Tactic: 0x00000000000003ea Time: 0.00955307
Tactic: 0x0000000000000000 Time: 0.00676224
Fastest Tactic: 0x00000000000003e8 Time: 0.00663948
*************** Autotuning Reformat: Float(1600,1,160,16) -> Float(1600,100,10,1) ***************
--------------- Timing Runner: Optimizer Reformat(15 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00662086
Tactic: 0x00000000000003ea Time: 0.0150483
Tactic: 0x0000000000000000 Time: 0.00678784
Fastest Tactic: 0x00000000000003e8 Time: 0.00662086
*************** Autotuning Reformat: Float(1600,1,160,16) -> Float(400,1:4,40,4) ***************
--------------- Timing Runner: Optimizer Reformat(15 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.0067264
Tactic: 0x00000000000003ea Time: 0.0148647
Tactic: 0x0000000000000000 Time: 0.00673792
Fastest Tactic: 0x00000000000003e8 Time: 0.0067264
*************** Autotuning Reformat: Float(400,1:4,40,4) -> Float(1600,100,10,1) ***************
--------------- Timing Runner: Optimizer Reformat(15 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00661061
Tactic: 0x00000000000003ea Time: 0.0150975
Tactic: 0x0000000000000000 Time: 0.00670827
Fastest Tactic: 0x00000000000003e8 Time: 0.00661061
=============== Computing reformatting costs
*************** Autotuning Reformat: Float(400,25,5,1) -> Float(400,1,80,16) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00673536
Tactic: 0x00000000000003ea Time: 0.00941185
Tactic: 0x0000000000000000 Time: 0.00664952
Fastest Tactic: 0x0000000000000000 Time: 0.00664952
*************** Autotuning Reformat: Float(400,25,5,1) -> Float(100,1:4,20,4) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00652144
Tactic: 0x00000000000003ea Time: 0.00959269
Tactic: 0x0000000000000000 Time: 0.0066355
Fastest Tactic: 0x00000000000003e8 Time: 0.00652144
*************** Autotuning Reformat: Float(400,25,5,1) -> Float(25,25:32,5,1) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00702022
Tactic: 0x00000000000003ea Time: 0.0144373
Tactic: 0x0000000000000000 Time: 0.00706244
Fastest Tactic: 0x00000000000003e8 Time: 0.00702022
*************** Autotuning Reformat: Float(400,1,80,16) -> Float(400,25,5,1) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00652841
Tactic: 0x00000000000003ea Time: 0.0144378
Tactic: 0x0000000000000000 Time: 0.00669355
Fastest Tactic: 0x00000000000003e8 Time: 0.00652841
*************** Autotuning Reformat: Float(400,1,80,16) -> Float(100,1:4,20,4) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00648164
Tactic: 0x00000000000003ea Time: 0.0141191
Tactic: 0x0000000000000000 Time: 0.00658656
Fastest Tactic: 0x00000000000003e8 Time: 0.00648164
*************** Autotuning Reformat: Float(400,1,80,16) -> Float(25,25:32,5,1) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00640282
Tactic: 0x00000000000003ea Time: 0.0122088
Tactic: 0x0000000000000000 Time: 0.00644718
Fastest Tactic: 0x00000000000003e8 Time: 0.00640282
*************** Autotuning Reformat: Float(100,1:4,20,4) -> Float(400,25,5,1) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00622044
Tactic: 0x00000000000003ea Time: 0.0125148
Tactic: 0x0000000000000000 Time: 0.00606681
Fastest Tactic: 0x0000000000000000 Time: 0.00606681
*************** Autotuning Reformat: Float(100,1:4,20,4) -> Float(400,1,80,16) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00599638
Tactic: 0x00000000000003ea Time: 0.0122411
Tactic: 0x0000000000000000 Time: 0.00612577
Fastest Tactic: 0x00000000000003e8 Time: 0.00599638
*************** Autotuning Reformat: Float(100,1:4,20,4) -> Float(25,25:32,5,1) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.0062323
Tactic: 0x00000000000003ea Time: 0.0122065
Tactic: 0x0000000000000000 Time: 0.00636377
Fastest Tactic: 0x00000000000003e8 Time: 0.0062323
*************** Autotuning Reformat: Float(25,25:32,5,1) -> Float(400,25,5,1) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00598781
Tactic: 0x00000000000003ea Time: 0.0125045
Tactic: 0x0000000000000000 Time: 0.00583044
Fastest Tactic: 0x0000000000000000 Time: 0.00583044
*************** Autotuning Reformat: Float(25,25:32,5,1) -> Float(400,1,80,16) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00572149
Tactic: 0x00000000000003ea Time: 0.0124354
Tactic: 0x0000000000000000 Time: 0.00615699
Fastest Tactic: 0x00000000000003e8 Time: 0.00572149
*************** Autotuning Reformat: Float(25,25:32,5,1) -> Float(100,1:4,20,4) ***************
--------------- Timing Runner: Optimizer Reformat(16 -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.0062323
Tactic: 0x00000000000003ea Time: 0.012248
Tactic: 0x0000000000000000 Time: 0.00651282
Fastest Tactic: 0x00000000000003e8 Time: 0.0062323
=============== Computing reformatting costs
*************** Autotuning Reformat: Float(400,1,1,1) -> Float(400,1,400,400) ***************
--------------- Timing Runner: Optimizer Reformat(reshape_before_Gemm_7_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00621274
Tactic: 0x00000000000003ea Time: 0.00962408
Tactic: 0x0000000000000000 Time: 0.00616223
Fastest Tactic: 0x0000000000000000 Time: 0.00616223
*************** Autotuning Reformat: Float(400,1,1,1) -> Float(100,1:4,100,100) ***************
--------------- Timing Runner: Optimizer Reformat(reshape_before_Gemm_7_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00615409
Tactic: 0x00000000000003ea Time: 0.0116543
Tactic: 0x0000000000000000 Time: 0.00635371
Fastest Tactic: 0x00000000000003e8 Time: 0.00615409
*************** Autotuning Reformat: Float(400,1,400,400) -> Float(400,1,1,1) ***************
--------------- Timing Runner: Optimizer Reformat(reshape_before_Gemm_7_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00614807
Tactic: 0x00000000000003ea Time: 0.00932711
Tactic: 0x0000000000000000 Time: 0.0290274
Fastest Tactic: 0x00000000000003e8 Time: 0.00614807
*************** Autotuning Reformat: Float(400,1,400,400) -> Float(100,1:4,100,100) ***************
--------------- Timing Runner: Optimizer Reformat(reshape_before_Gemm_7_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.0062001
Tactic: 0x00000000000003ea Time: 0.0118073
Tactic: 0x0000000000000000 Time: 0.00629253
Fastest Tactic: 0x00000000000003e8 Time: 0.0062001
*************** Autotuning Reformat: Float(100,1:4,100,100) -> Float(400,1,1,1) ***************
--------------- Timing Runner: Optimizer Reformat(reshape_before_Gemm_7_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00599505
Tactic: 0x00000000000003ea Time: 0.0118547
Tactic: 0x0000000000000000 Time: 0.00622044
Fastest Tactic: 0x00000000000003e8 Time: 0.00599505
*************** Autotuning Reformat: Float(100,1:4,100,100) -> Float(400,1,400,400) ***************
--------------- Timing Runner: Optimizer Reformat(reshape_before_Gemm_7_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00617076
Tactic: 0x00000000000003ea Time: 0.0116576
Tactic: 0x0000000000000000 Time: 0.00627872
Fastest Tactic: 0x00000000000003e8 Time: 0.00617076
*************** Autotuning Reformat: Float(13,1:32,1,1) -> Float(400,1,1,1) ***************
--------------- Timing Runner: Optimizer Reformat(reshape_before_Gemm_7_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00614458
Tactic: 0x00000000000003ea Time: 0.011787
Tactic: 0x0000000000000000 Time: 0.00618193
Fastest Tactic: 0x00000000000003e8 Time: 0.00614458
*************** Autotuning Reformat: Float(13,1:32,1,1) -> Float(400,1,400,400) ***************
--------------- Timing Runner: Optimizer Reformat(reshape_before_Gemm_7_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00615195
Tactic: 0x00000000000003ea Time: 0.0116506
Tactic: 0x0000000000000000 Time: 0.006192
Fastest Tactic: 0x00000000000003e8 Time: 0.00615195
*************** Autotuning Reformat: Float(13,1:32,1,1) -> Float(100,1:4,100,100) ***************
--------------- Timing Runner: Optimizer Reformat(reshape_before_Gemm_7_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00627635
Tactic: 0x00000000000003ea Time: 0.0116502
Tactic: 0x0000000000000000 Time: 0.00624691
Fastest Tactic: 0x0000000000000000 Time: 0.00624691
=============== Computing reformatting costs
*************** Autotuning Reformat: Float(120,1,1,1) -> Float(120,1,120,120) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_8_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00611821
Tactic: 0x00000000000003ea Time: 0.0116616
Tactic: 0x0000000000000000 Time: 0.00613217
Fastest Tactic: 0x00000000000003e8 Time: 0.00611821
*************** Autotuning Reformat: Float(120,1,1,1) -> Float(30,1:4,30,30) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_8_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00627832
Tactic: 0x00000000000003ea Time: 0.0116988
Tactic: 0x0000000000000000 Time: 0.00623546
Fastest Tactic: 0x0000000000000000 Time: 0.00623546
*************** Autotuning Reformat: Float(120,1,120,120) -> Float(120,1,1,1) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_8_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00611782
Tactic: 0x00000000000003ea Time: 0.011712
Tactic: 0x0000000000000000 Time: 0.00618054
Fastest Tactic: 0x00000000000003e8 Time: 0.00611782
*************** Autotuning Reformat: Float(120,1,120,120) -> Float(30,1:4,30,30) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_8_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00618884
Tactic: 0x00000000000003ea Time: 0.0116289
Tactic: 0x0000000000000000 Time: 0.0062398
Fastest Tactic: 0x00000000000003e8 Time: 0.00618884
*************** Autotuning Reformat: Float(30,1:4,30,30) -> Float(120,1,1,1) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_8_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00613876
Tactic: 0x00000000000003ea Time: 0.0112839
Tactic: 0x0000000000000000 Time: 0.00612693
Fastest Tactic: 0x0000000000000000 Time: 0.00612693
*************** Autotuning Reformat: Float(30,1:4,30,30) -> Float(120,1,120,120) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_8_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00607302
Tactic: 0x00000000000003ea Time: 0.011162
Tactic: 0x0000000000000000 Time: 0.006192
Fastest Tactic: 0x00000000000003e8 Time: 0.00607302
=============== Computing reformatting costs
*************** Autotuning Reformat: Float(84,1,1,1) -> Float(84,1,84,84) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_10_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00605562
Tactic: 0x00000000000003ea Time: 0.0112196
Tactic: 0x0000000000000000 Time: 0.00613236
Fastest Tactic: 0x00000000000003e8 Time: 0.00605562
*************** Autotuning Reformat: Float(84,1,1,1) -> Float(21,1:4,21,21) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_10_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00610288
Tactic: 0x00000000000003ea Time: 0.0112356
Tactic: 0x0000000000000000 Time: 0.00619496
Fastest Tactic: 0x00000000000003e8 Time: 0.00610288
*************** Autotuning Reformat: Float(84,1,84,84) -> Float(84,1,1,1) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_10_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00620049
Tactic: 0x00000000000003ea Time: 0.0112359
Tactic: 0x0000000000000000 Time: 0.00612383
Fastest Tactic: 0x0000000000000000 Time: 0.00612383
*************** Autotuning Reformat: Float(84,1,84,84) -> Float(21,1:4,21,21) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_10_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00608737
Tactic: 0x00000000000003ea Time: 0.0111698
Tactic: 0x0000000000000000 Time: 0.00623546
Fastest Tactic: 0x00000000000003e8 Time: 0.00608737
*************** Autotuning Reformat: Float(21,1:4,21,21) -> Float(84,1,1,1) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_10_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00603486
Tactic: 0x00000000000003ea Time: 0.0112416
Tactic: 0x0000000000000000 Time: 0.00618805
Fastest Tactic: 0x00000000000003e8 Time: 0.00603486
*************** Autotuning Reformat: Float(21,1:4,21,21) -> Float(84,1,84,84) ***************
--------------- Timing Runner: Optimizer Reformat(Relu_10_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00610133
Tactic: 0x00000000000003ea Time: 0.0111804
Tactic: 0x0000000000000000 Time: 0.00611588
Fastest Tactic: 0x00000000000003e8 Time: 0.00610133
=============== Computing reformatting costs
*************** Autotuning Reformat: Float(10,1,10,10) -> Float(10,1,1,1) ***************
--------------- Timing Runner: Optimizer Reformat(Gemm_11_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00601428
Tactic: 0x00000000000003ea Time: 0.0112494
Tactic: 0x0000000000000000 Time: 0.00605105
Fastest Tactic: 0x00000000000003e8 Time: 0.00601428
*************** Autotuning Reformat: Float(3,1:4,3,3) -> Float(10,1,1,1) ***************
--------------- Timing Runner: Optimizer Reformat(Gemm_11_out_tensor -> <out>) (Reformat)
Tactic: 0x00000000000003e8 Time: 0.00604686
Tactic: 0x00000000000003ea Time: 0.0112388
Tactic: 0x0000000000000000 Time: 0.00610967
Fastest Tactic: 0x00000000000003e8 Time: 0.00604686
=============== Computing reformatting costs
=============== Computing costs for 
*************** Autotuning format combination: Float(1024,1024,32,1) -> Float(4704,784,28,1) ***************
--------------- Timing Runner: Conv_0 + Relu_1 (CudaDepthwiseConvolution)
CudaDepthwiseConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Conv_0 + Relu_1 (FusedConvActConvolution)
FusedConvActConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Conv_0 + Relu_1 (CudnnConvolution)
Tactic: 0x0000000000000000 Time: 0.021362
Tactic: 0x0000000000000001 Time: 0.0166481
Tactic: 0x0000000000000002 Time: 0.0160091
Tactic: 0x0000000000000004 skipped. Scratch requested: 1672192, available: 1048576
Tactic: 0x0000000000000005 Time: 0.0442027
Tactic: 0x0000000000000038 Time: 0.0166456
Tactic: 0x0000000000000039 Time: 0.0166034
Tactic: 0x000000000000003a Time: 0.0160508
Tactic: 0x000000000000003c skipped. Scratch requested: 1672192, available: 1048576
Tactic: 0x000000000000003d Time: 0.0504716
Tactic: 0x0000000000000070 Time: 0.0166303
Tactic: 0x0000000000000071 Time: 0.0166471
Tactic: 0x0000000000000072 Time: 0.016032
Tactic: 0x0000000000000074 skipped. Scratch requested: 1672192, available: 1048576
Tactic: 0x0000000000000075 Time: 0.0421947
Some tactics do not have sufficient workspace memory to run. Increasing workspace size will enable more tactics, please check verbose output for requested sizes.
Fastest Tactic: 0x0000000000000002 Time: 0.0160091
Setting workspace to 1672192enables more tactics for profiling
--------------- Timing Runner: Conv_0 + Relu_1 (CaskConvolution)
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
Tactic: 0x12dbf7d94ee3696d Time: 0.0177488
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
Tactic: 0x3f243c490d502deb Time: 0.0139467
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
Tactic: 0x503619c69ae500ff Time: 0.0169563
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
Tactic: 0x5403ad713f811a18 Time: 0.0172128
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
Tactic: 0x5aa723e0481da855 Time: 0.0172848
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
Tactic: 0x5deb29b7a8e275f7 Time: 0.0143156
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r5s5_aligna4_alignc4 Tactic: 0x967f94e9db2001a8
Tactic: 0x967f94e9db2001a8 Time: 0.0117274
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
Tactic: 0x9808072e706def96 Time: 0.0135134
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
Tactic: 0xa31d27de74b895ff Time: 0.0126084
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
Tactic: 0xa8609adc4e0ceb90 Time: 0.0136845
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
Tactic: 0xa8ef60e712f8ad24 Time: 0.0168091
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
Tactic: 0xbb8c3889c7eacd30 Time: 0.0348171
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0135834
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
Tactic: 0xd828f024626fa982 Time: 0.0133692
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
Tactic: 0xf067e6205da31c2e Time: 0.0169643
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
Tactic: 0xf64396b97c889179 Time: 0.0141916
Fastest Tactic: 0x967f94e9db2001a8 Time: 0.0117274
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x967f94e9db2001a8
*************** Autotuning format combination: Float(1024,1,32,1) -> Float(4704,1,168,6) ***************
--------------- Timing Runner: Conv_0 + Relu_1 (CaskConvolution)
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
Tactic: 0x19b688348f983aa0 Time: 0.020758
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b
Tactic: 0x1acd4f006848c62b Time: 0.0098218
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
Tactic: 0x1da91d865428f237 Time: 0.033119
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32
Tactic: 0x22cadc265a3b2e32 Time: 0.00955185
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087
Tactic: 0x43ffe5cf09cee087 Time: 0.00792087
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r5s5_aligna4_alignc4 Tactic: 0x45ff0c3a09b02e78
Tactic: 0x45ff0c3a09b02e78 Time: 0.0201707
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
Tactic: 0x62835fce994f06dd Time: 0.0206036
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1
Tactic: 0x7e40882e33c8fbf1 Time: 0.0083213
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
Tactic: 0x8014228ec08b4d49 Time: 0.0595378
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2
Tactic: 0xcf8ea142095f02d2 Time: 0.00794133
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4
Tactic: 0xf231cca3335919a4 Time: 0.00845947
Fastest Tactic: 0x43ffe5cf09cee087 Time: 0.00792087
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43ffe5cf09cee087
*************** Autotuning format combination: Float(1024,1:4,32,1) -> Float(1568,1:4,56,2) ***************
--------------- Timing Runner: Conv_0 + Relu_1 (CaskConvolution)
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde
Tactic: 0x10383a0781d24dde Time: 0.00854865
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
Tactic: 0x112d7e4d280f396d Time: 0.0250636
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
Tactic: 0x13cc2ad8dcd32ba2 Time: 0.0262851
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
Tactic: 0x17173deba0b64484 Time: 0.0465707
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
Tactic: 0x19b688348f983aa0 Time: 0.0208151
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028
Tactic: 0x1a373db9a2bc4028 Time: 0.00892463
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b
Tactic: 0x1acd4f006848c62b Time: 0.0135846
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
Tactic: 0x1da91d865428f237 Time: 0.0331064
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x20ec60dfb4198123
Tactic: 0x20ec60dfb4198123 Time: 0.0273912
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32
Tactic: 0x22cadc265a3b2e32 Time: 0.0130851
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
Tactic: 0x25b2b9d5c9d5ca0d Time: 0.0341312
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
Tactic: 0x27b316f52c109002 Time: 0.0487939
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x2c48dee9745d6bd1
Tactic: 0x2c48dee9745d6bd1 Time: 0.0531535
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
Tactic: 0x36303f399d3ee4fd Time: 0.0622578
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
Tactic: 0x385bfab9332b1413 Time: 0.0298338
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582
Tactic: 0x3a8712b17741b582 Time: 0.00907128
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
Tactic: 0x3e191488237fab8f Time: 0.0489935
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
Tactic: 0x3e2b881168d9689d Time: 0.048704
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067
Tactic: 0x3f948a101b8c4067 Time: 0.0103431
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422
Tactic: 0x4037b478ce77e422 Time: 0.00837706
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
Tactic: 0x412c44dfeaf9161d Time: 0.0486811
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087
Tactic: 0x43ffe5cf09cee087 Time: 0.0132911
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r5s5_aligna4_alignc4 Tactic: 0x45ff0c3a09b02e78
Tactic: 0x45ff0c3a09b02e78 Time: 0.0203049
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700
Tactic: 0x4640eb34c8ecc700 Time: 0.0101883
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
Tactic: 0x482b48242255b8ce Time: 0.0524952
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
Tactic: 0x5030121339a48bf3 Time: 0.046904
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0
Tactic: 0x570667f2a28165a0 Time: 0.0114414
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
Tactic: 0x62835fce994f06dd Time: 0.0205835
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
Tactic: 0x634e99502974e4da Time: 0.0465573
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
Tactic: 0x65e41d81f093b482 Time: 0.0257633
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
Tactic: 0x65f71607d20d5438 Time: 0.024109
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
Tactic: 0x6716429226d146f7 Time: 0.0250766
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x6af69a7346d256e4
Tactic: 0x6af69a7346d256e4 Time: 0.0221113
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x6c223b9bb59c4911
Tactic: 0x6c223b9bb59c4911 Time: 0.0501257
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
Tactic: 0x6e2a3c7a7fc5e4e1 Time: 0.0225266
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef
Tactic: 0x72a5d05b1bb165ef Time: 0.0103829
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x77cea0eece03cdc8
Tactic: 0x77cea0eece03cdc8 Time: 0.0240686
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
Tactic: 0x7bc32c782b800c48 Time: 0.0461853
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76
Tactic: 0x7bff86d5f2eadc76 Time: 0.0106763
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1
Tactic: 0x7e40882e33c8fbf1 Time: 0.0129797
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
Tactic: 0x8014228ec08b4d49 Time: 0.0595289
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
Tactic: 0x818413b69874bb35 Time: 0.0278871
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9
Tactic: 0x93030576a9fb03f9 Time: 0.011655
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798
Tactic: 0x9355e195cee05798 Time: 0.0112153
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x94f2b2320bb3fc74
Tactic: 0x94f2b2320bb3fc74 Time: 0.0235733
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d
Tactic: 0x96467934a22da27d Time: 0.00892182
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
Tactic: 0x998c97842e775a17 Time: 0.0517806
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221
Tactic: 0x9cb304e2edbc1221 Time: 0.0104074
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0xa4ccdf6a03ea9ede
Tactic: 0xa4ccdf6a03ea9ede Time: 0.0357429
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0
Tactic: 0xab0496509b88ebe0 Time: 0.0103205
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
Tactic: 0xae0c89d047932ba3 Time: 0.0462707
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd
Tactic: 0xae48d3ccfe1edfcd Time: 0.00898077
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64
Tactic: 0xb33296dda7141c64 Time: 0.00893137
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232
Tactic: 0xb3e5ce9d1b1da232 Time: 0.0134118
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
Tactic: 0xb443c221fcb1565b Time: 0.0243726
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
Tactic: 0xb47d7d926de02dee Time: 0.0420827
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7
Tactic: 0xb6f6563c77d057d7 Time: 0.00932622
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
Tactic: 0xba0185279ccb2b85 Time: 0.0415644
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
Tactic: 0xbdfdef6b84f7ccc9 Time: 0.048099
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
Tactic: 0xc0a715d897e240bb Time: 0.040563
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
Tactic: 0xc7feb33970feefa7 Time: 0.0480716
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
Tactic: 0xca7eeb8d9143d738 Time: 0.0478583
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677
Tactic: 0xcc46f0f5cee60677 Time: 0.00817093
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
Tactic: 0xcedbed6d66c946d0 Time: 0.0376533
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2
Tactic: 0xcf8ea142095f02d2 Time: 0.0130991
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0xd44c0150ab60e391
Tactic: 0xd44c0150ab60e391 Time: 0.0354411
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
Tactic: 0xd7f5d575d49a4bc7 Time: 0.0246316
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
Tactic: 0xd9031472c05adf51 Time: 0.0471813
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0xd92e381f65c40faf
Tactic: 0xd92e381f65c40faf Time: 0.0238499
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5
Tactic: 0xdb77237fa21087f5 Time: 0.0106923
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
Tactic: 0xe47307053a42b3e4 Time: 0.045988
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0xe60966add848b046
Tactic: 0xe60966add848b046 Time: 0.0385908
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
Tactic: 0xe797e099911c0624 Time: 0.0555596
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638
Tactic: 0xe9e5475c77d60638 Time: 0.0104349
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4
Tactic: 0xf231cca3335919a4 Time: 0.0132098
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
Tactic: 0xf4156675c5f728d4 Time: 0.043884
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23
Tactic: 0xf78ec258f27b3e23 Time: 0.0108879
Conv_0 + Relu_1 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
Tactic: 0xf90060ce8193b811 Time: 0.046204
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba
Tactic: 0xfed494d61b2087ba Time: 0.010634
Fastest Tactic: 0xcc46f0f5cee60677 Time: 0.00817093
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xcc46f0f5cee60677
=============== Computing costs for 
*************** Autotuning format combination: Float(4704,784,28,1) -> Float(1176,196,14,1) ***************
--------------- Timing Runner: MaxPool_2 (TiledPooling)
Tactic: 0x0000000000540101 Time: 0.00895018
Tactic: 0x0000000000550101 Time: 0.00851147
Tactic: 0x0000000000560101 Time: 0.00864985
Tactic: 0x0000000000570101 Time: 0.00851147
Tactic: 0x0000000000580101 Time: 0.00873545
Tactic: 0x0000000000590101 Time: 0.00848
Tactic: 0x00000000005a0101 Time: 0.00855631
Tactic: 0x00000000005b0101 Time: 0.008464
Tactic: 0x00000000005c0101 Time: 0.00889993
Tactic: 0x00000000005d0101 Time: 0.00839893
Tactic: 0x00000000005e0101 Time: 0.0085192
Tactic: 0x00000000005f0101 Time: 0.00839067
Tactic: 0x0000000000600101 Time: 0.008468
Tactic: 0x0000000000610101 Time: 0.008544
Tactic: 0x0000000000620101 Time: 0.00869087
Tactic: 0x0000000000630101 Time: 0.00752521
Fastest Tactic: 0x0000000000630101 Time: 0.00752521
--------------- Timing Runner: MaxPool_2 (CudnnPooling)
Tactic: 0xffffffffffffffff Time: 0.00898274
Fastest Tactic: 0xffffffffffffffff Time: 0.00898274
--------------- Timing Runner: MaxPool_2 (CaskPooling)
MaxPool_2 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: 0xb59f9cfb90407c92
Tactic: 0xb59f9cfb90407c92 Time: 0.00839493
Fastest Tactic: 0xb59f9cfb90407c92 Time: 0.00839493
>>>>>>>>>>>>>>> Chose Runner Type: TiledPooling Tactic: 0x0000000000630101
*************** Autotuning format combination: Float(1568,1:4,56,2) -> Float(392,1:4,28,2) ***************
--------------- Timing Runner: MaxPool_2 (CaskPooling)
MaxPool_2 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Max_CAlign4 Tactic: 0x22fb1bb4a70e340d
Tactic: 0x22fb1bb4a70e340d Time: 0.00632634
Fastest Tactic: 0x22fb1bb4a70e340d Time: 0.00632634
>>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x22fb1bb4a70e340d
=============== Computing costs for 
*************** Autotuning format combination: Float(1176,196,14,1) -> Float(1600,100,10,1) ***************
--------------- Timing Runner: Conv_3 + Relu_4 (CudaDepthwiseConvolution)
CudaDepthwiseConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Conv_3 + Relu_4 (FusedConvActConvolution)
FusedConvActConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Conv_3 + Relu_4 (CudnnConvolution)
Tactic: 0x0000000000000000 Time: 0.021238
Tactic: 0x0000000000000001 Time: 0.021238
Tactic: 0x0000000000000002 Time: 0.0288524
Tactic: 0x0000000000000004 skipped. Scratch requested: 1189376, available: 1048576
Tactic: 0x0000000000000005 skipped. Scratch requested: 3481600, available: 1048576
Tactic: 0x0000000000000038 Time: 0.0212473
Tactic: 0x0000000000000039 Time: 0.0212227
Tactic: 0x000000000000003a Time: 0.0288071
Tactic: 0x000000000000003c skipped. Scratch requested: 1189376, available: 1048576
Tactic: 0x000000000000003d skipped. Scratch requested: 3481600, available: 1048576
Tactic: 0x0000000000000070 Time: 0.021234
Tactic: 0x0000000000000071 Time: 0.021252
Tactic: 0x0000000000000072 Time: 0.0289022
Tactic: 0x0000000000000074 skipped. Scratch requested: 1189376, available: 1048576
Tactic: 0x0000000000000075 skipped. Scratch requested: 3481600, available: 1048576
Fastest Tactic: 0x0000000000000039 Time: 0.0212227
Setting workspace to 1189376enables more tactics for profiling
--------------- Timing Runner: Conv_3 + Relu_4 (CaskConvolution)
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x12dbf7d94ee3696d
Tactic: 0x12dbf7d94ee3696d Time: 0.0315462
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
Tactic: 0x3f243c490d502deb Time: 0.0267339
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
Tactic: 0x503619c69ae500ff Time: 0.0393244
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 0x5403ad713f811a18
Tactic: 0x5403ad713f811a18 Time: 0.0397902
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x5aa723e0481da855
Tactic: 0x5aa723e0481da855 Time: 0.0295662
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 0x5deb29b7a8e275f7
Tactic: 0x5deb29b7a8e275f7 Time: 0.027639
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r5s5_aligna4_alignc4 Tactic: 0x967f94e9db2001a8
Tactic: 0x967f94e9db2001a8 Time: 0.0192041
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
Tactic: 0x9808072e706def96 Time: 0.0296044
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xa31d27de74b895ff
Tactic: 0xa31d27de74b895ff Time: 0.0199711
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
Tactic: 0xa8609adc4e0ceb90 Time: 0.0304737
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
Tactic: 0xa8ef60e712f8ad24 Time: 0.0386524
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_aligna4_alignc4 Tactic: 0xbb8c3889c7eacd30
Tactic: 0xbb8c3889c7eacd30 Time: 0.0824427
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
Tactic: 0xc3cf6e1d1c6aff27 Time: 0.025783
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0xd828f024626fa982
Tactic: 0xd828f024626fa982 Time: 0.0269202
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
Tactic: 0xf067e6205da31c2e Time: 0.0392664
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
Tactic: 0xf64396b97c889179 Time: 0.0275011
Fastest Tactic: 0x967f94e9db2001a8 Time: 0.0192041
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x967f94e9db2001a8
*************** Autotuning format combination: Float(1176,1,84,6) -> Float(1600,1,160,16) ***************
--------------- Timing Runner: Conv_3 + Relu_4 (CaskConvolution)
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
Tactic: 0x17173deba0b64484 Time: 0.0462427
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
Tactic: 0x19b688348f983aa0 Time: 0.0204242
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b
Tactic: 0x1acd4f006848c62b Time: 0.0169616
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
Tactic: 0x1da91d865428f237 Time: 0.0330618
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32
Tactic: 0x22cadc265a3b2e32 Time: 0.016321
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067
Tactic: 0x3f948a101b8c4067 Time: 0.0112896
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087
Tactic: 0x43ffe5cf09cee087 Time: 0.0147075
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r5s5_aligna4_alignc4 Tactic: 0x45ff0c3a09b02e78
Tactic: 0x45ff0c3a09b02e78 Time: 0.0199586
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700
Tactic: 0x4640eb34c8ecc700 Time: 0.0111531
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
Tactic: 0x5030121339a48bf3 Time: 0.0468373
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0
Tactic: 0x570667f2a28165a0 Time: 0.0136772
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
Tactic: 0x62835fce994f06dd Time: 0.0202177
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76
Tactic: 0x7bff86d5f2eadc76 Time: 0.012416
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1
Tactic: 0x7e40882e33c8fbf1 Time: 0.0163337
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
Tactic: 0x8014228ec08b4d49 Time: 0.0599947
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9
Tactic: 0x93030576a9fb03f9 Time: 0.0139102
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798
Tactic: 0x9355e195cee05798 Time: 0.0134127
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0
Tactic: 0xab0496509b88ebe0 Time: 0.0121352
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232
Tactic: 0xb3e5ce9d1b1da232 Time: 0.0151646
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
Tactic: 0xca7eeb8d9143d738 Time: 0.0476724
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2
Tactic: 0xcf8ea142095f02d2 Time: 0.0148563
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
Tactic: 0xd9031472c05adf51 Time: 0.0470613
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5
Tactic: 0xdb77237fa21087f5 Time: 0.0123923
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638
Tactic: 0xe9e5475c77d60638 Time: 0.0122518
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4
Tactic: 0xf231cca3335919a4 Time: 0.0164302
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23
Tactic: 0xf78ec258f27b3e23 Time: 0.0126609
Fastest Tactic: 0x4640eb34c8ecc700 Time: 0.0111531
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x4640eb34c8ecc700
*************** Autotuning format combination: Float(392,1:4,28,2) -> Float(400,1:4,40,4) ***************
--------------- Timing Runner: Conv_3 + Relu_4 (CaskConvolution)
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x10383a0781d24dde
Tactic: 0x10383a0781d24dde Time: 0.0106887
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x112d7e4d280f396d
Tactic: 0x112d7e4d280f396d Time: 0.0246865
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x13cc2ad8dcd32ba2
Tactic: 0x13cc2ad8dcd32ba2 Time: 0.0262375
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
Tactic: 0x17173deba0b64484 Time: 0.04648
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x19b688348f983aa0
Tactic: 0x19b688348f983aa0 Time: 0.0202114
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0x1a373db9a2bc4028
Tactic: 0x1a373db9a2bc4028 Time: 0.0111456
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x1acd4f006848c62b
Tactic: 0x1acd4f006848c62b Time: 0.0203689
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x1da91d865428f237
Tactic: 0x1da91d865428f237 Time: 0.0330773
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x20ec60dfb4198123
Tactic: 0x20ec60dfb4198123 Time: 0.0270523
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x22cadc265a3b2e32
Tactic: 0x22cadc265a3b2e32 Time: 0.0194655
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x25b2b9d5c9d5ca0d
Tactic: 0x25b2b9d5c9d5ca0d Time: 0.0341227
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
Tactic: 0x27b316f52c109002 Time: 0.0486369
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x2c48dee9745d6bd1
Tactic: 0x2c48dee9745d6bd1 Time: 0.0529996
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0x36303f399d3ee4fd
Tactic: 0x36303f399d3ee4fd Time: 0.06216
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x385bfab9332b1413
Tactic: 0x385bfab9332b1413 Time: 0.0294853
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x3a8712b17741b582
Tactic: 0x3a8712b17741b582 Time: 0.011652
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
Tactic: 0x3e191488237fab8f Time: 0.0488579
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
Tactic: 0x3e2b881168d9689d Time: 0.0485806
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x3f948a101b8c4067
Tactic: 0x3f948a101b8c4067 Time: 0.0131852
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x4037b478ce77e422
Tactic: 0x4037b478ce77e422 Time: 0.0105523
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
Tactic: 0x412c44dfeaf9161d Time: 0.0485608
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087
Tactic: 0x43ffe5cf09cee087 Time: 0.0174091
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r5s5_aligna4_alignc4 Tactic: 0x45ff0c3a09b02e78
Tactic: 0x45ff0c3a09b02e78 Time: 0.0196543
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x4640eb34c8ecc700
Tactic: 0x4640eb34c8ecc700 Time: 0.0132094
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x482b48242255b8ce
Tactic: 0x482b48242255b8ce Time: 0.0523657
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
Tactic: 0x5030121339a48bf3 Time: 0.0468387
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0x570667f2a28165a0
Tactic: 0x570667f2a28165a0 Time: 0.0151277
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_aligna4_alignc4 Tactic: 0x62835fce994f06dd
Tactic: 0x62835fce994f06dd Time: 0.0201606
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0x634e99502974e4da
Tactic: 0x634e99502974e4da Time: 0.047268
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65e41d81f093b482
Tactic: 0x65e41d81f093b482 Time: 0.0256697
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x65f71607d20d5438
Tactic: 0x65f71607d20d5438 Time: 0.0236018
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6716429226d146f7
Tactic: 0x6716429226d146f7 Time: 0.0251208
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x6af69a7346d256e4
Tactic: 0x6af69a7346d256e4 Time: 0.0217473
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x6c223b9bb59c4911
Tactic: 0x6c223b9bb59c4911 Time: 0.0499215
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x6e2a3c7a7fc5e4e1
Tactic: 0x6e2a3c7a7fc5e4e1 Time: 0.0222649
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x72a5d05b1bb165ef
Tactic: 0x72a5d05b1bb165ef Time: 0.0139018
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x77cea0eece03cdc8
Tactic: 0x77cea0eece03cdc8 Time: 0.0240076
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
Tactic: 0x7bc32c782b800c48 Time: 0.046144
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc8 Tactic: 0x7bff86d5f2eadc76
Tactic: 0x7bff86d5f2eadc76 Time: 0.0143213
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x7e40882e33c8fbf1
Tactic: 0x7e40882e33c8fbf1 Time: 0.0182091
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_aligna4_alignc4 Tactic: 0x8014228ec08b4d49
Tactic: 0x8014228ec08b4d49 Time: 0.0599751
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0x818413b69874bb35
Tactic: 0x818413b69874bb35 Time: 0.0275307
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0x93030576a9fb03f9
Tactic: 0x93030576a9fb03f9 Time: 0.0164419
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0x9355e195cee05798
Tactic: 0x9355e195cee05798 Time: 0.0158979
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0x94f2b2320bb3fc74
Tactic: 0x94f2b2320bb3fc74 Time: 0.0234852
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc8 Tactic: 0x96467934a22da27d
Tactic: 0x96467934a22da27d Time: 0.0110727
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8 Tactic: 0x998c97842e775a17
Tactic: 0x998c97842e775a17 Time: 0.0517074
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0x9cb304e2edbc1221
Tactic: 0x9cb304e2edbc1221 Time: 0.0139556
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0xa4ccdf6a03ea9ede
Tactic: 0xa4ccdf6a03ea9ede Time: 0.035664
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xab0496509b88ebe0
Tactic: 0xab0496509b88ebe0 Time: 0.0141431
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
Tactic: 0xae0c89d047932ba3 Time: 0.046
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xae48d3ccfe1edfcd
Tactic: 0xae48d3ccfe1edfcd Time: 0.0110393
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8 Tactic: 0xb33296dda7141c64
Tactic: 0xb33296dda7141c64 Time: 0.0116237
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4 Tactic: 0xb3e5ce9d1b1da232
Tactic: 0xb3e5ce9d1b1da232 Time: 0.0179514
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb443c221fcb1565b
Tactic: 0xb443c221fcb1565b Time: 0.0244853
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xb47d7d926de02dee
Tactic: 0xb47d7d926de02dee Time: 0.0419867
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xb6f6563c77d057d7
Tactic: 0xb6f6563c77d057d7 Time: 0.0117973
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xba0185279ccb2b85
Tactic: 0xba0185279ccb2b85 Time: 0.0415218
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 0xbdfdef6b84f7ccc9
Tactic: 0xbdfdef6b84f7ccc9 Time: 0.0480061
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xc0a715d897e240bb
Tactic: 0xc0a715d897e240bb Time: 0.0405653
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
Tactic: 0xc7feb33970feefa7 Time: 0.0477516
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: 0xca7eeb8d9143d738
Tactic: 0xca7eeb8d9143d738 Time: 0.047296
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677
Tactic: 0xcc46f0f5cee60677 Time: 0.0101563
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xcedbed6d66c946d0
Tactic: 0xcedbed6d66c946d0 Time: 0.0375893
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xcf8ea142095f02d2
Tactic: 0xcf8ea142095f02d2 Time: 0.0173904
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0xd44c0150ab60e391
Tactic: 0xd44c0150ab60e391 Time: 0.0353397
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xd7f5d575d49a4bc7
Tactic: 0xd7f5d575d49a4bc7 Time: 0.0241882
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
Tactic: 0xd9031472c05adf51 Time: 0.0468813
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0xd92e381f65c40faf
Tactic: 0xd92e381f65c40faf Time: 0.0232683
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xdb77237fa21087f5
Tactic: 0xdb77237fa21087f5 Time: 0.0143502
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
Tactic: 0xe47307053a42b3e4 Time: 0.045676
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r5s5 Tactic: 0xe60966add848b046
Tactic: 0xe60966add848b046 Time: 0.038637
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8 Tactic: 0xe797e099911c0624
Tactic: 0xe797e099911c0624 Time: 0.0554621
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8 Tactic: 0xe9e5475c77d60638
Tactic: 0xe9e5475c77d60638 Time: 0.0142111
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc4 Tactic: 0xf231cca3335919a4
Tactic: 0xf231cca3335919a4 Time: 0.0184561
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 0xf4156675c5f728d4
Tactic: 0xf4156675c5f728d4 Time: 0.0436387
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna8_alignc4 Tactic: 0xf78ec258f27b3e23
Tactic: 0xf78ec258f27b3e23 Time: 0.0144596
Conv_3 + Relu_4 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
Tactic: 0xf90060ce8193b811 Time: 0.0466413
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x32x16_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xfed494d61b2087ba
Tactic: 0xfed494d61b2087ba Time: 0.0141689
Fastest Tactic: 0xcc46f0f5cee60677 Time: 0.0101563
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xcc46f0f5cee60677
=============== Computing costs for 
*************** Autotuning format combination: Float(1600,100,10,1) -> Float(400,25,5,1) ***************
--------------- Timing Runner: MaxPool_5 (TiledPooling)
Tactic: 0x0000000000540101 Time: 0.00733797
Tactic: 0x0000000000550101 Time: 0.0073338
Tactic: 0x0000000000560101 Time: 0.00714984
Tactic: 0x0000000000570101 Time: 0.00711943
Tactic: 0x0000000000580101 Time: 0.00711512
Tactic: 0x0000000000590101 Time: 0.00745339
Tactic: 0x00000000005a0101 Time: 0.00721362
Tactic: 0x00000000005b0101 Time: 0.00717458
Tactic: 0x00000000005c0101 Time: 0.0071453
Tactic: 0x00000000005d0101 Time: 0.00719728
Tactic: 0x00000000005e0101 Time: 0.00716255
Tactic: 0x00000000005f0101 Time: 0.00713123
Tactic: 0x0000000000600101 Time: 0.00701533
Tactic: 0x0000000000610101 Time: 0.00709333
Tactic: 0x0000000000620101 Time: 0.00727374
Tactic: 0x0000000000630101 Time: 0.00710196
Fastest Tactic: 0x0000000000600101 Time: 0.00701533
--------------- Timing Runner: MaxPool_5 (CudnnPooling)
Tactic: 0xffffffffffffffff Time: 0.00816598
Fastest Tactic: 0xffffffffffffffff Time: 0.00816598
--------------- Timing Runner: MaxPool_5 (CaskPooling)
MaxPool_5 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: 0xb59f9cfb90407c92
Tactic: 0xb59f9cfb90407c92 Time: 0.00663592
Fastest Tactic: 0xb59f9cfb90407c92 Time: 0.00663592
>>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb59f9cfb90407c92
*************** Autotuning format combination: Float(400,1:4,40,4) -> Float(100,1:4,20,4) ***************
--------------- Timing Runner: MaxPool_5 (CaskPooling)
MaxPool_5 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Max_CAlign4 Tactic: 0x22fb1bb4a70e340d
Tactic: 0x22fb1bb4a70e340d Time: 0.00662756
Fastest Tactic: 0x22fb1bb4a70e340d Time: 0.00662756
>>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x22fb1bb4a70e340d
=============== Computing costs for 
*************** Autotuning format combination: Float(400,25,5,1) -> Float(400,1,1,1) ***************
--------------- Timing Runner: Flatten_6 + reshape_before_Gemm_7 (Shuffle)
Tactic: 0x0000000000000000 Time: 0.00802235
Tactic: 0x0000000000000001 Time: 0.0242004
Fastest Tactic: 0x0000000000000000 Time: 0.00802235
>>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000
*************** Autotuning format combination: Float(400,1,80,16) -> Float(400,1,400,400) ***************
--------------- Timing Runner: Flatten_6 + reshape_before_Gemm_7 (Shuffle)
Tactic: 0x0000000000000000 Time: 0.0071335
Tactic: 0x0000000000000001 Time: 0.0246057
Fastest Tactic: 0x0000000000000000 Time: 0.0071335
>>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000
*************** Autotuning format combination: Float(100,1:4,20,4) -> Float(100,1:4,100,100) ***************
--------------- Timing Runner: Flatten_6 + reshape_before_Gemm_7 (Shuffle)
Tactic: 0x0000000000000000 Time: 0.00717118
Tactic: 0x0000000000000001 Time: 0.0243779
Fastest Tactic: 0x0000000000000000 Time: 0.00717118
>>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000
*************** Autotuning format combination: Float(25,25:32,5,1) -> Float(13,1:32,1,1) ***************
--------------- Timing Runner: Flatten_6 + reshape_before_Gemm_7 (Shuffle)
Tactic: 0x0000000000000000 Time: 0.00717118
Tactic: 0x0000000000000001 Time: 0.0243566
Fastest Tactic: 0x0000000000000000 Time: 0.00717118
>>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000
=============== Computing costs for 
*************** Autotuning format combination: Float(400,1,1,1) -> Float(120,1,1,1) ***************
--------------- Timing Runner: Gemm_7 + Relu_8 (CudaDepthwiseConvolution)
CudaDepthwiseConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_7 + Relu_8 (FusedConvActConvolution)
FusedConvActConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_7 + Relu_8 (CudnnConvolution)
Tactic: 0x0000000000000000 Time: 0.0334507
Tactic: 0x0000000000000001 Time: 0.0179542
Tactic: 0x0000000000000002 Time: 0.117461
Tactic: 0x0000000000000004 skipped. Scratch requested: 140275200, available: 1048576
Tactic: 0x0000000000000005 skipped. Scratch requested: 8791040, available: 1048576
Tactic: 0x0000000000000038 Time: 0.0334165
Tactic: 0x0000000000000039 Time: 0.0178959
Tactic: 0x000000000000003a Time: 0.117429
Tactic: 0x000000000000003c skipped. Scratch requested: 140275200, available: 1048576
Tactic: 0x000000000000003d skipped. Scratch requested: 8791040, available: 1048576
Tactic: 0x0000000000000070 Time: 0.0334603
Tactic: 0x0000000000000071 Time: 0.0334304
Tactic: 0x0000000000000072 Time: 0.117426
Tactic: 0x0000000000000074 skipped. Scratch requested: 140275200, available: 1048576
Tactic: 0x0000000000000075 skipped. Scratch requested: 8791040, available: 1048576
Fastest Tactic: 0x0000000000000039 Time: 0.0178959
Setting workspace to 8791040enables more tactics for profiling
Try increasing the workspace size to 4194304 bytes to get better performance.
--------------- Timing Runner: Gemm_7 + Relu_8 (CublasConvolution)
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000000 Time: 0.0175264
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000001 Time: 0.0175545
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000002 Time: 0.0154841
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000003 Time: 0.0151777
Fastest Tactic: 0x0000000000000003 Time: 0.0151777
--------------- Timing Runner: Gemm_7 + Relu_8 (CaskConvolution)
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x089fce8f59784020
Tactic: 0x089fce8f59784020 Time: 0.0237369
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x09aeee2f1beac383
Tactic: 0x09aeee2f1beac383 Time: 0.0180688
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1016909b1fd6ff2f
Tactic: 0x1016909b1fd6ff2f Time: 0.0286684
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x126c151fbf444788
Tactic: 0x126c151fbf444788 Time: 0.0193801
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
Tactic: 0x1fc87d7eb370bb7a Time: 0.0362656
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x20109de926a12263
Tactic: 0x20109de926a12263 Time: 0.043888
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
Tactic: 0x2ee10e11d6651675 Time: 0.103669
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x37f7c166cac77c6f
Tactic: 0x37f7c166cac77c6f Time: 0.0294889
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
Tactic: 0x3f243c490d502deb Time: 0.0529158
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x44429615b0244d08
Tactic: 0x44429615b0244d08 Time: 0.0312029
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
Tactic: 0x503619c69ae500ff Time: 0.086056
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x5104686f63bb0267
Tactic: 0x5104686f63bb0267 Time: 0.0569493
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x55952bdf9713ff4e
Tactic: 0x55952bdf9713ff4e Time: 0.040173
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x562e86db2b553d73
Tactic: 0x562e86db2b553d73 Time: 0.041549
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5a37b78cd3b8724f
Tactic: 0x5a37b78cd3b8724f Time: 0.0315704
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5d4d781b2e89fd83
Tactic: 0x5d4d781b2e89fd83 Time: 0.0400201
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5f5015dbf902f0a3
Tactic: 0x5f5015dbf902f0a3 Time: 0.0234375
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5fc609cfe7c5b7d9
Tactic: 0x5fc609cfe7c5b7d9 Time: 0.0207642
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x6fb0aa4a23519390
Tactic: 0x6fb0aa4a23519390 Time: 0.0448413
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
Tactic: 0x7f0145cb49517338 Time: 0.0352299
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x84ce4136c9dcccb4
Tactic: 0x84ce4136c9dcccb4 Time: 0.0508114
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
Tactic: 0x865894c4635db7fd Time: 0.0348341
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
Tactic: 0x8e3884f0eaec3ecd Time: 0.0588604
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
Tactic: 0x9808072e706def96 Time: 0.0577831
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
Tactic: 0x9cd5cdc35441c505 Time: 0.0554438
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
Tactic: 0x9de226a0c44627c4 Time: 0.103216
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
Tactic: 0xa419b3b68f2da07b Time: 0.03384
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
Tactic: 0xa8609adc4e0ceb90 Time: 0.0597902
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
Tactic: 0xa8ef60e712f8ad24 Time: 0.083904
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
Tactic: 0xc0b05b61d128e46e Time: 0.0559289
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0512396
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0xd97544c2f30ecbf9
Tactic: 0xd97544c2f30ecbf9 Time: 0.0441293
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
Tactic: 0xe5603263b7f00303 Time: 0.0602489
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0xed454ebf201fb878
Tactic: 0xed454ebf201fb878 Time: 0.0219867
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
Tactic: 0xf067e6205da31c2e Time: 0.084912
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
Tactic: 0xf64396b97c889179 Time: 0.0556604
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
Tactic: 0xfff46c7893896eb1 Time: 0.176325
Fastest Tactic: 0x09aeee2f1beac383 Time: 0.0180688
>>>>>>>>>>>>>>> Chose Runner Type: CublasConvolution Tactic: 0x0000000000000003
*************** Autotuning format combination: Float(400,1,400,400) -> Float(120,1,120,120) ***************
--------------- Timing Runner: Gemm_7 + Relu_8 (CublasConvolution)
CublasConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_7 + Relu_8 (CaskConvolution)
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
Tactic: 0x1022069e6f8d9aeb Time: 0.050432
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
Tactic: 0x130df49cb195156b Time: 0.0230194
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
Tactic: 0x17173deba0b64484 Time: 0.0834293
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x233399c4b9cc77c1
Tactic: 0x233399c4b9cc77c1 Time: 0.0217187
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
Tactic: 0x27b316f52c109002 Time: 0.0471427
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2b4a5a2f546acbfd
Tactic: 0x2b4a5a2f546acbfd Time: 0.0149454
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
Tactic: 0x35f26f9c09557d86 Time: 0.0504381
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
Tactic: 0x3e191488237fab8f Time: 0.0290471
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
Tactic: 0x3e2b881168d9689d Time: 0.0287929
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
Tactic: 0x412c44dfeaf9161d Time: 0.0470987
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x46e3f4ab19d1017d
Tactic: 0x46e3f4ab19d1017d Time: 0.0124496
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x4aa9c14ea2943ef0
Tactic: 0x4aa9c14ea2943ef0 Time: 0.0159451
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x4ddd867d1da71041
Tactic: 0x4ddd867d1da71041 Time: 0.0209613
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
Tactic: 0x5030121339a48bf3 Time: 0.0848
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x53b347fe11460a8e
Tactic: 0x53b347fe11460a8e Time: 0.0227072
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
Tactic: 0x55d80c17b1cd982d Time: 0.028056
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x591671c3dd9388cc
Tactic: 0x591671c3dd9388cc Time: 0.0306027
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x5b317e1cbcf34d49
Tactic: 0x5b317e1cbcf34d49 Time: 0.0302871
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x5ed17eb1dfe2e6b0
Tactic: 0x5ed17eb1dfe2e6b0 Time: 0.0153295
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x61f62003626e5959
Tactic: 0x61f62003626e5959 Time: 0.0170416
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
Tactic: 0x7bc32c782b800c48 Time: 0.083624
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x85222248e9a8aead
Tactic: 0x85222248e9a8aead Time: 0.010695
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
Tactic: 0x90898977fc8ce537 Time: 0.0275438
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
Tactic: 0x9dece0dc37e90462 Time: 0.0227243
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xa71326710e3f683c
Tactic: 0xa71326710e3f683c Time: 0.0126953
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xabb79847ce7b82ce
Tactic: 0xabb79847ce7b82ce Time: 0.0308179
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
Tactic: 0xae0c89d047932ba3 Time: 0.0459147
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xba269390692faead
Tactic: 0xba269390692faead Time: 0.0149162
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xba76ca882b17caa6
Tactic: 0xba76ca882b17caa6 Time: 0.0225138
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
Tactic: 0xbc0bba0ff1a92939 Time: 0.09232
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
Tactic: 0xc7b3afceb5fb03c0 Time: 0.0496213
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
Tactic: 0xc7feb33970feefa7 Time: 0.0286071
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
Tactic: 0xd55ee6fd0b56f808 Time: 0.0501029
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
Tactic: 0xd9031472c05adf51 Time: 0.0840933
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
Tactic: 0xe47307053a42b3e4 Time: 0.0821589
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xebdd7d350fbaa00e
Tactic: 0xebdd7d350fbaa00e Time: 0.0307462
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xed09dcddfcf4bffb
Tactic: 0xed09dcddfcf4bffb Time: 0.0116579
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xf031e640742524d7
Tactic: 0xf031e640742524d7 Time: 0.0153571
Gemm_7 + Relu_8 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
Tactic: 0xf90060ce8193b811 Time: 0.083976
Fastest Tactic: 0x85222248e9a8aead Time: 0.010695
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85222248e9a8aead
*************** Autotuning format combination: Float(100,1:4,100,100) -> Float(30,1:4,30,30) ***************
--------------- Timing Runner: Gemm_7 + Relu_8 (CublasConvolution)
CublasConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_7 + Relu_8 (CaskConvolution)
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
Tactic: 0x130df49cb195156b Time: 0.0230073
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x233399c4b9cc77c1
Tactic: 0x233399c4b9cc77c1 Time: 0.0216967
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2b4a5a2f546acbfd
Tactic: 0x2b4a5a2f546acbfd Time: 0.0149588
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x46e3f4ab19d1017d
Tactic: 0x46e3f4ab19d1017d Time: 0.0124792
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x4aa9c14ea2943ef0
Tactic: 0x4aa9c14ea2943ef0 Time: 0.0159619
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x4ddd867d1da71041
Tactic: 0x4ddd867d1da71041 Time: 0.020908
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x53b347fe11460a8e
Tactic: 0x53b347fe11460a8e Time: 0.0227008
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x591671c3dd9388cc
Tactic: 0x591671c3dd9388cc Time: 0.030561
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x5b317e1cbcf34d49
Tactic: 0x5b317e1cbcf34d49 Time: 0.0302204
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x5ed17eb1dfe2e6b0
Tactic: 0x5ed17eb1dfe2e6b0 Time: 0.0153319
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x61f62003626e5959
Tactic: 0x61f62003626e5959 Time: 0.0170245
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x85222248e9a8aead
Tactic: 0x85222248e9a8aead Time: 0.0108146
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
Tactic: 0x9dece0dc37e90462 Time: 0.0226923
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xa71326710e3f683c
Tactic: 0xa71326710e3f683c Time: 0.0127008
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xabb79847ce7b82ce
Tactic: 0xabb79847ce7b82ce Time: 0.0307801
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xba269390692faead
Tactic: 0xba269390692faead Time: 0.0149241
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xba76ca882b17caa6
Tactic: 0xba76ca882b17caa6 Time: 0.022464
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xebdd7d350fbaa00e
Tactic: 0xebdd7d350fbaa00e Time: 0.0307336
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xed09dcddfcf4bffb
Tactic: 0xed09dcddfcf4bffb Time: 0.0116668
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xf031e640742524d7
Tactic: 0xf031e640742524d7 Time: 0.0153556
Fastest Tactic: 0x85222248e9a8aead Time: 0.0108146
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85222248e9a8aead
=============== Computing costs for 
*************** Autotuning format combination: Float(120,1,1,1) -> Float(84,1,1,1) ***************
--------------- Timing Runner: Gemm_9 + Relu_10 (CudaDepthwiseConvolution)
CudaDepthwiseConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_9 + Relu_10 (FusedConvActConvolution)
FusedConvActConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_9 + Relu_10 (CudnnConvolution)
Tactic: 0x0000000000000000 Time: 0.0191129
Tactic: 0x0000000000000001 Time: 0.0145943
Tactic: 0x0000000000000002 Time: 0.0453973
Tactic: 0x0000000000000004 skipped. Scratch requested: 32112128, available: 1048576
Tactic: 0x0000000000000005 skipped. Scratch requested: 2258944, available: 1048576
Tactic: 0x0000000000000038 Time: 0.0191182
Tactic: 0x0000000000000039 Time: 0.0145614
Tactic: 0x000000000000003a Time: 0.0454107
Tactic: 0x000000000000003c skipped. Scratch requested: 32112128, available: 1048576
Tactic: 0x000000000000003d skipped. Scratch requested: 2258944, available: 1048576
Tactic: 0x0000000000000070 Time: 0.0191046
Tactic: 0x0000000000000071 Time: 0.0191425
Tactic: 0x0000000000000072 Time: 0.0454173
Tactic: 0x0000000000000074 skipped. Scratch requested: 32112128, available: 1048576
Tactic: 0x0000000000000075 skipped. Scratch requested: 2258944, available: 1048576
Fastest Tactic: 0x0000000000000039 Time: 0.0145614
Setting workspace to 2258944enables more tactics for profiling
Try increasing the workspace size to 4194304 bytes to get better performance.
--------------- Timing Runner: Gemm_9 + Relu_10 (CublasConvolution)
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000000 Time: 0.0133465
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000001 Time: 0.0134302
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000002 Time: 0.0129194
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000003 Time: 0.0145758
Fastest Tactic: 0x0000000000000002 Time: 0.0129194
--------------- Timing Runner: Gemm_9 + Relu_10 (CaskConvolution)
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x089fce8f59784020
Tactic: 0x089fce8f59784020 Time: 0.0171749
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x09aeee2f1beac383
Tactic: 0x09aeee2f1beac383 Time: 0.0119627
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1016909b1fd6ff2f
Tactic: 0x1016909b1fd6ff2f Time: 0.0194696
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x126c151fbf444788
Tactic: 0x126c151fbf444788 Time: 0.0131906
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
Tactic: 0x1fc87d7eb370bb7a Time: 0.0168907
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x20109de926a12263
Tactic: 0x20109de926a12263 Time: 0.0262597
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
Tactic: 0x2ee10e11d6651675 Time: 0.0459333
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x37f7c166cac77c6f
Tactic: 0x37f7c166cac77c6f Time: 0.0203827
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
Tactic: 0x3f243c490d502deb Time: 0.0231275
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x44429615b0244d08
Tactic: 0x44429615b0244d08 Time: 0.0217067
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
Tactic: 0x503619c69ae500ff Time: 0.0333042
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x5104686f63bb0267
Tactic: 0x5104686f63bb0267 Time: 0.0350795
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x55952bdf9713ff4e
Tactic: 0x55952bdf9713ff4e Time: 0.0270146
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x562e86db2b553d73
Tactic: 0x562e86db2b553d73 Time: 0.0248762
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5a37b78cd3b8724f
Tactic: 0x5a37b78cd3b8724f Time: 0.0219933
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5d4d781b2e89fd83
Tactic: 0x5d4d781b2e89fd83 Time: 0.0237518
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5f5015dbf902f0a3
Tactic: 0x5f5015dbf902f0a3 Time: 0.0167259
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5fc609cfe7c5b7d9
Tactic: 0x5fc609cfe7c5b7d9 Time: 0.0142769
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x6fb0aa4a23519390
Tactic: 0x6fb0aa4a23519390 Time: 0.0310652
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
Tactic: 0x7f0145cb49517338 Time: 0.0243642
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x84ce4136c9dcccb4
Tactic: 0x84ce4136c9dcccb4 Time: 0.030559
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
Tactic: 0x865894c4635db7fd Time: 0.0166298
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
Tactic: 0x8e3884f0eaec3ecd Time: 0.0262663
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
Tactic: 0x9808072e706def96 Time: 0.023904
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
Tactic: 0x9cd5cdc35441c505 Time: 0.0250347
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
Tactic: 0x9de226a0c44627c4 Time: 0.0443493
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
Tactic: 0xa419b3b68f2da07b Time: 0.022954
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
Tactic: 0xa8609adc4e0ceb90 Time: 0.0247291
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
Tactic: 0xa8ef60e712f8ad24 Time: 0.0326458
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
Tactic: 0xc0b05b61d128e46e Time: 0.0254629
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0225394
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0xd97544c2f30ecbf9
Tactic: 0xd97544c2f30ecbf9 Time: 0.0264738
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
Tactic: 0xe5603263b7f00303 Time: 0.0272911
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0xed454ebf201fb878
Tactic: 0xed454ebf201fb878 Time: 0.0155903
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
Tactic: 0xf067e6205da31c2e Time: 0.033055
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
Tactic: 0xf64396b97c889179 Time: 0.0239413
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
Tactic: 0xfff46c7893896eb1 Time: 0.0645653
Fastest Tactic: 0x09aeee2f1beac383 Time: 0.0119627
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x09aeee2f1beac383
*************** Autotuning format combination: Float(120,1,120,120) -> Float(84,1,84,84) ***************
--------------- Timing Runner: Gemm_9 + Relu_10 (CublasConvolution)
CublasConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_9 + Relu_10 (CaskConvolution)
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
Tactic: 0x1022069e6f8d9aeb Time: 0.0230052
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
Tactic: 0x130df49cb195156b Time: 0.0141547
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 0x17173deba0b64484
Tactic: 0x17173deba0b64484 Time: 0.0310982
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x233399c4b9cc77c1
Tactic: 0x233399c4b9cc77c1 Time: 0.0129157
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x27b316f52c109002
Tactic: 0x27b316f52c109002 Time: 0.0213933
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2b4a5a2f546acbfd
Tactic: 0x2b4a5a2f546acbfd Time: 0.0101876
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
Tactic: 0x35f26f9c09557d86 Time: 0.0232064
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x3e191488237fab8f
Tactic: 0x3e191488237fab8f Time: 0.0153561
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0x3e2b881168d9689d
Tactic: 0x3e2b881168d9689d Time: 0.0151549
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x412c44dfeaf9161d
Tactic: 0x412c44dfeaf9161d Time: 0.0214153
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x46e3f4ab19d1017d
Tactic: 0x46e3f4ab19d1017d Time: 0.00869607
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x4aa9c14ea2943ef0
Tactic: 0x4aa9c14ea2943ef0 Time: 0.0107988
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x4ddd867d1da71041
Tactic: 0x4ddd867d1da71041 Time: 0.01224
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 0x5030121339a48bf3
Tactic: 0x5030121339a48bf3 Time: 0.0317954
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x53b347fe11460a8e
Tactic: 0x53b347fe11460a8e Time: 0.0144462
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
Tactic: 0x55d80c17b1cd982d Time: 0.0143764
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x591671c3dd9388cc
Tactic: 0x591671c3dd9388cc Time: 0.0166981
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x5b317e1cbcf34d49
Tactic: 0x5b317e1cbcf34d49 Time: 0.0165623
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x5ed17eb1dfe2e6b0
Tactic: 0x5ed17eb1dfe2e6b0 Time: 0.010623
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x61f62003626e5959
Tactic: 0x61f62003626e5959 Time: 0.0118069
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 0x7bc32c782b800c48
Tactic: 0x7bc32c782b800c48 Time: 0.0315355
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x85222248e9a8aead
Tactic: 0x85222248e9a8aead Time: 0.00882554
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
Tactic: 0x90898977fc8ce537 Time: 0.0139587
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
Tactic: 0x9dece0dc37e90462 Time: 0.0139316
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xa71326710e3f683c
Tactic: 0xa71326710e3f683c Time: 0.00893249
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xabb79847ce7b82ce
Tactic: 0xabb79847ce7b82ce Time: 0.0169781
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xae0c89d047932ba3
Tactic: 0xae0c89d047932ba3 Time: 0.0207611
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xba269390692faead
Tactic: 0xba269390692faead Time: 0.0102552
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xba76ca882b17caa6
Tactic: 0xba76ca882b17caa6 Time: 0.0141791
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
Tactic: 0xbc0bba0ff1a92939 Time: 0.0388184
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
Tactic: 0xc7b3afceb5fb03c0 Time: 0.0226816
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xc7feb33970feefa7
Tactic: 0xc7feb33970feefa7 Time: 0.0150043
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
Tactic: 0xd55ee6fd0b56f808 Time: 0.0229305
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: 0xd9031472c05adf51
Tactic: 0xd9031472c05adf51 Time: 0.0316044
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: 0xe47307053a42b3e4
Tactic: 0xe47307053a42b3e4 Time: 0.0310759
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xebdd7d350fbaa00e
Tactic: 0xebdd7d350fbaa00e Time: 0.0168805
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xed09dcddfcf4bffb
Tactic: 0xed09dcddfcf4bffb Time: 0.0084256
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xf031e640742524d7
Tactic: 0xf031e640742524d7 Time: 0.0106607
Gemm_9 + Relu_10 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 0xf90060ce8193b811
Tactic: 0xf90060ce8193b811 Time: 0.031551
Fastest Tactic: 0xed09dcddfcf4bffb Time: 0.0084256
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xed09dcddfcf4bffb
*************** Autotuning format combination: Float(30,1:4,30,30) -> Float(21,1:4,21,21) ***************
--------------- Timing Runner: Gemm_9 + Relu_10 (CublasConvolution)
CublasConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_9 + Relu_10 (CaskConvolution)
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
Tactic: 0x130df49cb195156b Time: 0.0141813
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x233399c4b9cc77c1
Tactic: 0x233399c4b9cc77c1 Time: 0.012935
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2b4a5a2f546acbfd
Tactic: 0x2b4a5a2f546acbfd Time: 0.0101792
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x46e3f4ab19d1017d
Tactic: 0x46e3f4ab19d1017d Time: 0.00868294
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x4aa9c14ea2943ef0
Tactic: 0x4aa9c14ea2943ef0 Time: 0.0108587
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x4ddd867d1da71041
Tactic: 0x4ddd867d1da71041 Time: 0.0122251
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x53b347fe11460a8e
Tactic: 0x53b347fe11460a8e Time: 0.0144476
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x591671c3dd9388cc
Tactic: 0x591671c3dd9388cc Time: 0.01668
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x5b317e1cbcf34d49
Tactic: 0x5b317e1cbcf34d49 Time: 0.0165257
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x5ed17eb1dfe2e6b0
Tactic: 0x5ed17eb1dfe2e6b0 Time: 0.010618
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x61f62003626e5959
Tactic: 0x61f62003626e5959 Time: 0.0118109
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x85222248e9a8aead
Tactic: 0x85222248e9a8aead Time: 0.00804572
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
Tactic: 0x9dece0dc37e90462 Time: 0.0139351
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xa71326710e3f683c
Tactic: 0xa71326710e3f683c Time: 0.0089353
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xabb79847ce7b82ce
Tactic: 0xabb79847ce7b82ce Time: 0.0170245
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xba269390692faead
Tactic: 0xba269390692faead Time: 0.0103008
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xba76ca882b17caa6
Tactic: 0xba76ca882b17caa6 Time: 0.0142102
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xebdd7d350fbaa00e
Tactic: 0xebdd7d350fbaa00e Time: 0.0168976
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xed09dcddfcf4bffb
Tactic: 0xed09dcddfcf4bffb Time: 0.0084024
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xf031e640742524d7
Tactic: 0xf031e640742524d7 Time: 0.010664
Fastest Tactic: 0x85222248e9a8aead Time: 0.00804572
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85222248e9a8aead
=============== Computing costs for 
*************** Autotuning format combination: Float(84,1,1,1) -> Float(10,1,1,1) ***************
--------------- Timing Runner: Gemm_11 (CudaDepthwiseConvolution)
CudaDepthwiseConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_11 (FusedConvActConvolution)
FusedConvActConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_11 (CudnnConvolution)
Tactic: 0x0000000000000000 Time: 0.0140142
Tactic: 0x0000000000000001 Time: 0.0104397
Tactic: 0x0000000000000002 Time: 0.0255543
Tactic: 0x0000000000000004 skipped. Scratch requested: 8132096, available: 1048576
Tactic: 0x0000000000000005 Time: 0.0451
Tactic: 0x0000000000000038 Time: 0.0140289
Tactic: 0x0000000000000039 Time: 0.010409
Tactic: 0x000000000000003a Time: 0.0255566
Tactic: 0x000000000000003c skipped. Scratch requested: 8132096, available: 1048576
Tactic: 0x000000000000003d Time: 0.0427747
Tactic: 0x0000000000000070 Time: 0.0139982
Tactic: 0x0000000000000071 Time: 0.0140249
Tactic: 0x0000000000000072 Time: 0.0255116
Tactic: 0x0000000000000074 skipped. Scratch requested: 8132096, available: 1048576
Tactic: 0x0000000000000075 Time: 0.043648
Fastest Tactic: 0x0000000000000039 Time: 0.010409
Setting workspace to 8132096enables more tactics for profiling
Try increasing the workspace size to 4194304 bytes to get better performance.
--------------- Timing Runner: Gemm_11 (CublasConvolution)
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000000 Time: 0.0114706
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000001 Time: 0.0123177
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000002 Time: 0.012277
Try increasing the workspace size to 4194304 bytes to get better performance.
Try increasing the workspace size to 4194304 bytes to get better performance.
Tactic: 0x0000000000000003 Time: 0.0122522
Fastest Tactic: 0x0000000000000000 Time: 0.0114706
--------------- Timing Runner: Gemm_11 (CaskConvolution)
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x089fce8f59784020
Tactic: 0x089fce8f59784020 Time: 0.0159137
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x09aeee2f1beac383
Tactic: 0x09aeee2f1beac383 Time: 0.0118915
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1016909b1fd6ff2f
Tactic: 0x1016909b1fd6ff2f Time: 0.0177353
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x126c151fbf444788
Tactic: 0x126c151fbf444788 Time: 0.013129
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x1fc87d7eb370bb7a
Tactic: 0x1fc87d7eb370bb7a Time: 0.014707
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x20109de926a12263
Tactic: 0x20109de926a12263 Time: 0.0241295
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x2ee10e11d6651675
Tactic: 0x2ee10e11d6651675 Time: 0.0390329
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x37f7c166cac77c6f
Tactic: 0x37f7c166cac77c6f Time: 0.0184612
Gemm_11 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 0x3f243c490d502deb
Tactic: 0x3f243c490d502deb Time: 0.0196618
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x44429615b0244d08
Tactic: 0x44429615b0244d08 Time: 0.0211873
Gemm_11 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 0x503619c69ae500ff
Tactic: 0x503619c69ae500ff Time: 0.0272558
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x5104686f63bb0267
Tactic: 0x5104686f63bb0267 Time: 0.0325857
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x55952bdf9713ff4e
Tactic: 0x55952bdf9713ff4e Time: 0.0255802
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x562e86db2b553d73
Tactic: 0x562e86db2b553d73 Time: 0.0230962
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5a37b78cd3b8724f
Tactic: 0x5a37b78cd3b8724f Time: 0.0214853
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5d4d781b2e89fd83
Tactic: 0x5d4d781b2e89fd83 Time: 0.02255
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5f5015dbf902f0a3
Tactic: 0x5f5015dbf902f0a3 Time: 0.0156465
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x5fc609cfe7c5b7d9
Tactic: 0x5fc609cfe7c5b7d9 Time: 0.0141529
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x6fb0aa4a23519390
Tactic: 0x6fb0aa4a23519390 Time: 0.0292853
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0x7f0145cb49517338
Tactic: 0x7f0145cb49517338 Time: 0.0228075
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0x84ce4136c9dcccb4
Tactic: 0x84ce4136c9dcccb4 Time: 0.0280996
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x865894c4635db7fd
Tactic: 0x865894c4635db7fd Time: 0.0144329
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x8e3884f0eaec3ecd
Tactic: 0x8e3884f0eaec3ecd Time: 0.0224427
Gemm_11 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: 0x9808072e706def96
Tactic: 0x9808072e706def96 Time: 0.0202064
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9cd5cdc35441c505
Tactic: 0x9cd5cdc35441c505 Time: 0.0214347
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x256x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x9de226a0c44627c4
Tactic: 0x9de226a0c44627c4 Time: 0.0375016
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1_aligna4_alignc4 Tactic: 0xa419b3b68f2da07b
Tactic: 0xa419b3b68f2da07b Time: 0.0214093
Gemm_11 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: 0xa8609adc4e0ceb90
Tactic: 0xa8609adc4e0ceb90 Time: 0.0207944
Gemm_11 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: 0xa8ef60e712f8ad24
Tactic: 0xa8ef60e712f8ad24 Time: 0.026839
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xc0b05b61d128e46e
Tactic: 0xc0b05b61d128e46e Time: 0.02193
Gemm_11 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: 0xc3cf6e1d1c6aff27
Tactic: 0xc3cf6e1d1c6aff27 Time: 0.0192676
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0xd97544c2f30ecbf9
Tactic: 0xd97544c2f30ecbf9 Time: 0.0248221
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0xe5603263b7f00303
Tactic: 0xe5603263b7f00303 Time: 0.0234681
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nchwkrsc_nchw_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1_aligna4_alignc4 Tactic: 0xed454ebf201fb878
Tactic: 0xed454ebf201fb878 Time: 0.0153842
Gemm_11 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: 0xf067e6205da31c2e
Tactic: 0xf067e6205da31c2e Time: 0.0271623
Gemm_11 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: 0xf64396b97c889179
Tactic: 0xf64396b97c889179 Time: 0.0203144
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nchwkcrs_nchw_tilesize256x128x8_stage3_warpsize4x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xfff46c7893896eb1
Tactic: 0xfff46c7893896eb1 Time: 0.0515337
Fastest Tactic: 0x09aeee2f1beac383 Time: 0.0118915
>>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 0x0000000000000039
*************** Autotuning format combination: Float(84,1,84,84) -> Float(10,1,10,10) ***************
--------------- Timing Runner: Gemm_11 (CublasConvolution)
CublasConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_11 (CaskConvolution)
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x1022069e6f8d9aeb
Tactic: 0x1022069e6f8d9aeb Time: 0.0199222
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x35f26f9c09557d86
Tactic: 0x35f26f9c09557d86 Time: 0.0199762
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_t1r1s1_aligna4_alignc4 Tactic: 0x55d80c17b1cd982d
Tactic: 0x55d80c17b1cd982d Time: 0.0127123
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x64x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0x90898977fc8ce537
Tactic: 0x90898977fc8ce537 Time: 0.0123848
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xbc0bba0ff1a92939
Tactic: 0xbc0bba0ff1a92939 Time: 0.0325382
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize64x128x8_stage3_warpsize1x4x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xc7b3afceb5fb03c0
Tactic: 0xc7b3afceb5fb03c0 Time: 0.0194477
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize128x64x8_stage3_warpsize2x2x1_g1_ffma_simple_t1r1s1_aligna4_alignc4 Tactic: 0xd55ee6fd0b56f808
Tactic: 0xd55ee6fd0b56f808 Time: 0.0196681
Fastest Tactic: 0x90898977fc8ce537 Time: 0.0123848
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x90898977fc8ce537
*************** Autotuning format combination: Float(21,1:4,21,21) -> Float(3,1:4,3,3) ***************
--------------- Timing Runner: Gemm_11 (CublasConvolution)
CublasConvolution has no valid tactics for this config, skipping
--------------- Timing Runner: Gemm_11 (CaskConvolution)
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x130df49cb195156b
Tactic: 0x130df49cb195156b Time: 0.0131073
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x233399c4b9cc77c1
Tactic: 0x233399c4b9cc77c1 Time: 0.0119189
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x2b4a5a2f546acbfd
Tactic: 0x2b4a5a2f546acbfd Time: 0.00941422
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x46e3f4ab19d1017d
Tactic: 0x46e3f4ab19d1017d Time: 0.00862222
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x4aa9c14ea2943ef0
Tactic: 0x4aa9c14ea2943ef0 Time: 0.0106557
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x4ddd867d1da71041
Tactic: 0x4ddd867d1da71041 Time: 0.0111879
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x53b347fe11460a8e
Tactic: 0x53b347fe11460a8e Time: 0.0134272
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x591671c3dd9388cc
Tactic: 0x591671c3dd9388cc Time: 0.0151568
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x5b317e1cbcf34d49
Tactic: 0x5b317e1cbcf34d49 Time: 0.0149394
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x5ed17eb1dfe2e6b0
Tactic: 0x5ed17eb1dfe2e6b0 Time: 0.00972434
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0x61f62003626e5959
Tactic: 0x61f62003626e5959 Time: 0.0116631
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x85222248e9a8aead
Tactic: 0x85222248e9a8aead Time: 0.00794057
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x9dece0dc37e90462
Tactic: 0x9dece0dc37e90462 Time: 0.0127407
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xa71326710e3f683c
Tactic: 0xa71326710e3f683c Time: 0.00884211
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xabb79847ce7b82ce
Tactic: 0xabb79847ce7b82ce Time: 0.0154521
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xba269390692faead
Tactic: 0xba269390692faead Time: 0.00941156
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0xba76ca882b17caa6
Tactic: 0xba76ca882b17caa6 Time: 0.0131647
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xebdd7d350fbaa00e
Tactic: 0xebdd7d350fbaa00e Time: 0.0152713
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xed09dcddfcf4bffb
Tactic: 0xed09dcddfcf4bffb Time: 0.00842214
Gemm_11 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 0xf031e640742524d7
Tactic: 0xf031e640742524d7 Time: 0.0099131
Fastest Tactic: 0x85222248e9a8aead Time: 0.00794057
>>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85222248e9a8aead
=============== Computing costs for 
*************** Autotuning format combination: Float(10,1,1,1) -> Float(10,1) ***************
--------------- Timing Runner: reshape_after_Gemm_11 (Shuffle)
Tactic: 0x0000000000000000 Time: 0.00636941
Tactic: 0x0000000000000001 Time: 0.0160747
Fastest Tactic: 0x0000000000000000 Time: 0.00636941
>>>>>>>>>>>>>>> Chose Runner Type: Shuffle Tactic: 0x0000000000000000
Adding reformat layer: Reformatted Input Tensor 0 to Conv_0 + Relu_1 (input) from Float(1024,1024,32,1) to Float(1024,1,32,1)
Adding reformat layer: Reformatted Input Tensor 0 to MaxPool_2 (12) from Float(4704,1,168,6) to Float(1568,1:4,56,2)
Adding reformat layer: Reformatted Input Tensor 0 to Flatten_6 + reshape_before_Gemm_7 (16) from Float(100,1:4,20,4) to Float(400,25,5,1)
Adding reformat layer: Reformatted Input Tensor 0 to Gemm_7 + Relu_8 (reshape_before_Gemm_7_out_tensor) from Float(400,1,1,1) to Float(400,1,400,400)
Adding reformat layer: Reformatted Input Tensor 0 to Gemm_9 + Relu_10 (Relu_8_out_tensor) from Float(120,1,120,120) to Float(30,1:4,30,30)
Adding reformat layer: Reformatted Input Tensor 0 to Gemm_11 (Relu_10_out_tensor) from Float(21,1:4,21,21) to Float(84,1,1,1)
Formats and tactics selection completed in 4.49291 seconds.
After reformat layers: 15 layers
Pre-optimized block assignment.
Block size 602112
Block size 200704
Block size 204800
Block size 51200
Block size 4
Block size 4
Block size 15360
Block size 10752
Block size 4
Block size 802816
Block size 51200
Block size 4
Block size 4
Block size 4
Block size 1048576
Total Activation Memory: 2987544
Detected 1 inputs and 1 output network tensors.
Conv_0 + Relu_1 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x8_aligna4_alignc8 Tactic: 0x43ffe5cf09cee087
MaxPool_2 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Max_CAlign4 Tactic: 0x22fb1bb4a70e340d
Conv_3 + Relu_4 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x8_alignc4 Tactic: 0xcc46f0f5cee60677
MaxPool_5 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NHWC_Max_CAlign4 Tactic: 0x22fb1bb4a70e340d
Gemm_7 + Relu_8 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x85222248e9a8aead
Gemm_9 + Relu_10 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: 0x85222248e9a8aead
Layer: Reformatting CopyNode for Input Tensor 0 to Conv_0 + Relu_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
Layer: Conv_0 + Relu_1 Host Persistent: 2496 Device Persistent: 0 Scratch Memory: 0
Layer: Reformatting CopyNode for Input Tensor 0 to MaxPool_2 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
Layer: MaxPool_2 Host Persistent: 1088 Device Persistent: 0 Scratch Memory: 0
Layer: Conv_3 + Relu_4 Host Persistent: 2496 Device Persistent: 0 Scratch Memory: 0
Layer: MaxPool_5 Host Persistent: 1088 Device Persistent: 0 Scratch Memory: 0
Layer: Reformatting CopyNode for Input Tensor 0 to Flatten_6 + reshape_before_Gemm_7 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
Layer: Flatten_6 + reshape_before_Gemm_7 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
Layer: Reformatting CopyNode for Input Tensor 0 to Gemm_7 + Relu_8 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
Layer: Gemm_7 + Relu_8 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
Layer: Reformatting CopyNode for Input Tensor 0 to Gemm_9 + Relu_10 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
Layer: Gemm_9 + Relu_10 Host Persistent: 2784 Device Persistent: 0 Scratch Memory: 0
Layer: Reformatting CopyNode for Input Tensor 0 to Gemm_11 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
Layer: Gemm_11 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 512
Layer: reshape_after_Gemm_11 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
Total Host Persistent Memory: 12768
Total Device Persistent Memory: 0
Total Scratch Memory: 512
[MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 0 MiB
[BlockAssignment] Algorithm ShiftNTopDown took 0.11526ms to assign 4 blocks to 15 nodes requiring 1404936 bytes.
Optimized block assignment.
Block size 802816
Block size 602112
Block size 4
Block size 4
Total Activation Memory: 1404936
Disabling unused tactic source: CUBLAS, CUBLAS_LT
Using cuDNN as a tactic source
[MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2015, GPU 3005 (MiB)
Engine generation completed in 5.49567 seconds.
Deleting timing cache: 75 entries, served 0 hits since creation.
Engine Layer Information:
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to Conv_0 + Relu_1, Tactic: 0x0000000000000000, input[Float(-2,1,32,32)] -> Reformatted Input Tensor 0 to Conv_0 + Relu_1[Float(-2,1,32,32)]
Layer(CaskConvolution): Conv_0 + Relu_1, Tactic: 0x43ffe5cf09cee087, Reformatted Input Tensor 0 to Conv_0 + Relu_1[Float(-2,1,32,32)] -> 12[Float(-2,6,28,28)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to MaxPool_2, Tactic: 0x00000000000003e8, 12[Float(-2,6,28,28)] -> Reformatted Input Tensor 0 to MaxPool_2[Float(-2,6,28,28)]
Layer(CaskPooling): MaxPool_2, Tactic: 0x22fb1bb4a70e340d, Reformatted Input Tensor 0 to MaxPool_2[Float(-2,6,28,28)] -> 13[Float(-2,6,14,14)]
Layer(CaskConvolution): Conv_3 + Relu_4, Tactic: 0xcc46f0f5cee60677, 13[Float(-2,6,14,14)] -> 15[Float(-2,16,10,10)]
Layer(CaskPooling): MaxPool_5, Tactic: 0x22fb1bb4a70e340d, 15[Float(-2,16,10,10)] -> 16[Float(-2,16,5,5)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to Flatten_6 + reshape_before_Gemm_7, Tactic: 0x0000000000000000, 16[Float(-2,16,5,5)] -> Reformatted Input Tensor 0 to Flatten_6 + reshape_before_Gemm_7[Float(-2,16,5,5)]
Layer(NoOp): Flatten_6 + reshape_before_Gemm_7, Tactic: 0x0000000000000000, Reformatted Input Tensor 0 to Flatten_6 + reshape_before_Gemm_7[Float(-2,16,5,5)] -> reshape_before_Gemm_7_out_region[Float(-2,400,1,1)]
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to Gemm_7 + Relu_8, Tactic: 0x0000000000000000, reshape_before_Gemm_7_out_region[Float(-2,400,1,1)] -> Reformatted Input Tensor 0 to Gemm_7 + Relu_8[Float(-2,400,1,1)]
Layer(CaskConvolution): Gemm_7 + Relu_8, Tactic: 0x85222248e9a8aead, Reformatted Input Tensor 0 to Gemm_7 + Relu_8[Float(-2,400,1,1)] -> Relu_8_out_tensor[Float(-2,120,1,1)]
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to Gemm_9 + Relu_10, Tactic: 0x0000000000000000, Relu_8_out_tensor[Float(-2,120,1,1)] -> Reformatted Input Tensor 0 to Gemm_9 + Relu_10[Float(-2,120,1,1)]
Layer(CaskConvolution): Gemm_9 + Relu_10, Tactic: 0x85222248e9a8aead, Reformatted Input Tensor 0 to Gemm_9 + Relu_10[Float(-2,120,1,1)] -> Relu_10_out_tensor[Float(-2,84,1,1)]
Layer(NoOp): Reformatting CopyNode for Input Tensor 0 to Gemm_11, Tactic: 0x0000000000000000, Relu_10_out_tensor[Float(-2,84,1,1)] -> Reformatted Input Tensor 0 to Gemm_11[Float(-2,84,1,1)]
Layer(CudnnConvolution): Gemm_11, Tactic: 0x0000000000000039, Reformatted Input Tensor 0 to Gemm_11[Float(-2,84,1,1)] -> Gemm_11_out_region[Float(-2,10,1,1)]
Layer(NoOp): reshape_after_Gemm_11, Tactic: 0x0000000000000000, Gemm_11_out_region[Float(-2,10,1,1)] -> output[Float(-2,10)]
[MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)
The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
Successful convert onnx to engine!
=====================================
=====================================
[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1598, GPU 2853 (MiB)
Loaded engine size: 0 MiB
Using cuDNN as a tactic source
[MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 1599, GPU 2863 (MiB)
Deserialization required 16431 microseconds.
[MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)
Input tensor dimensions: [-1, 1, 32, 32]
Output tensor dimensions: [-1, 10]
Here we set current batch to 2
Input element number: 2048
Output element number: 20
Using cuDNN as a tactic source
[MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 1599, GPU 2863 (MiB)
Total per-runner device persistent memory is 0
Total per-runner host persistent memory is 12768
Allocated activation device memory of size 1405952
[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)
output is:
-4.32606, -5.67475, -5.45733, 1.38097, 0.473019, 0.556213, -8.58306, 3.60364, -0.923273, 9.41911, 
1.97033, -5.90244, -4.43622, -4.80733, -1.35049, 0.547163, 12.5176, -8.10508, 0.0659186, -4.82802, 
The inference answer of LeNet5 is: 9
The inference answer of LeNet5 is: 6
