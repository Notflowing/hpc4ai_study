{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "         # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  \n",
    "        return self.w_o(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # Encode source\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        # Decode target with encoder output\n",
    "        decoder_output = self.decode(encoder_output, src_mask, tgt, tgt_mask)\n",
    "        # Project output to vocabulary size\n",
    "        output = self.project(decoder_output)\n",
    "        return output\n",
    "\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Transformer                                             [2, 20, 1000]             --\n",
       "├─InputEmbeddings: 1-1                                  [2, 20, 512]              --\n",
       "│    └─Embedding: 2-1                                   [2, 20, 512]              512,000\n",
       "├─PositionalEncoding: 1-2                               [2, 20, 512]              --\n",
       "│    └─Dropout: 2-2                                     [2, 20, 512]              --\n",
       "├─Encoder: 1-3                                          [2, 20, 512]              --\n",
       "│    └─ModuleList: 2-3                                  --                        --\n",
       "│    │    └─EncoderBlock: 3-1                           [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-5                        --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-1           [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-1      [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-2           [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-2                       [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-3                       [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-4                       [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-5                      [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-6                       [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-5                        --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-7           --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-2                 [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-8           [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-3      [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-4                  [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-9                       [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-10                     [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-11                      [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-5                        --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-12          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-4                 [2, 20, 512]              --\n",
       "│    │    └─EncoderBlock: 3-2                           [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-10                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-13          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-5      [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-7           [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-14                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-15                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-16                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-17                     [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-18                      [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-10                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-19          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-6                 [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-20          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-7      [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-9                  [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-21                      [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-22                     [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-23                      [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-10                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-24          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-8                 [2, 20, 512]              --\n",
       "│    │    └─EncoderBlock: 3-3                           [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-15                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-25          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-9      [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-12          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-26                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-27                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-28                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-29                     [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-30                      [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-15                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-31          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-10                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-32          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-11     [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-14                 [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-33                      [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-34                     [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-35                      [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-15                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-36          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-12                [2, 20, 512]              --\n",
       "│    │    └─EncoderBlock: 3-4                           [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-20                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-37          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-13     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-17          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-38                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-39                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-40                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-41                     [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-42                      [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-20                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-43          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-14                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-44          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-15     [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-19                 [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-45                      [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-46                     [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-47                      [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-20                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-48          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-16                [2, 20, 512]              --\n",
       "│    │    └─EncoderBlock: 3-5                           [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-25                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-49          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-17     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-22          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-50                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-51                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-52                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-53                     [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-54                      [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-25                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-55          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-18                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-56          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-19     [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-24                 [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-57                      [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-58                     [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-59                      [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-25                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-60          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-20                [2, 20, 512]              --\n",
       "│    │    └─EncoderBlock: 3-6                           [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-30                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-61          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-21     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-27          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-62                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-63                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-64                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-65                     [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-66                      [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-30                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-67          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-22                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-68          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-23     [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-29                 [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-69                      [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-70                     [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-71                      [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-30                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-72          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-24                [2, 20, 512]              --\n",
       "│    └─LayerNormalization: 2-4                          [2, 20, 512]              1,024\n",
       "├─InputEmbeddings: 1-4                                  [2, 20, 512]              --\n",
       "│    └─Embedding: 2-5                                   [2, 20, 512]              512,000\n",
       "├─PositionalEncoding: 1-5                               [2, 20, 512]              --\n",
       "│    └─Dropout: 2-6                                     [2, 20, 512]              --\n",
       "├─Decoder: 1-6                                          [2, 20, 512]              --\n",
       "│    └─ModuleList: 2-7                                  --                        --\n",
       "│    │    └─DecoderBlock: 3-7                           [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-37                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-73          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-25     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-32          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-74                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-75                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-76                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-77                     [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-78                      [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-37                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-79          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-26                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-80          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-27     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-34          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-81                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-82                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-83                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-84                     [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-85                      [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-37                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-86          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-28                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-87          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-29     [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-36                 [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-88                      [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-89                     [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-90                      [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-37                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-91          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-30                [2, 20, 512]              --\n",
       "│    │    └─DecoderBlock: 3-8                           [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-44                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-92          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-31     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-39          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-93                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-94                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-95                      [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-96                     [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-97                      [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-44                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-98          --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-32                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-99          [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-33     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-41          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-100                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-101                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-102                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-103                    [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-104                     [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-44                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-105         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-34                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-106         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-35     [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-43                 [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-107                     [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-108                    [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-109                     [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-44                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-110         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-36                [2, 20, 512]              --\n",
       "│    │    └─DecoderBlock: 3-9                           [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-51                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-111         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-37     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-46          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-112                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-113                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-114                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-115                    [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-116                     [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-51                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-117         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-38                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-118         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-39     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-48          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-119                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-120                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-121                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-122                    [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-123                     [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-51                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-124         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-40                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-125         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-41     [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-50                 [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-126                     [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-127                    [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-128                     [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-51                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-129         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-42                [2, 20, 512]              --\n",
       "│    │    └─DecoderBlock: 3-10                          [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-58                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-130         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-43     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-53          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-131                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-132                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-133                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-134                    [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-135                     [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-58                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-136         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-44                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-137         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-45     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-55          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-138                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-139                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-140                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-141                    [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-142                     [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-58                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-143         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-46                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-144         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-47     [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-57                 [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-145                     [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-146                    [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-147                     [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-58                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-148         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-48                [2, 20, 512]              --\n",
       "│    │    └─DecoderBlock: 3-11                          [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-65                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-149         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-49     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-60          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-150                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-151                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-152                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-153                    [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-154                     [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-65                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-155         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-50                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-156         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-51     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-62          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-157                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-158                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-159                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-160                    [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-161                     [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-65                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-162         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-52                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-163         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-53     [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-64                 [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-164                     [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-165                    [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-166                     [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-65                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-167         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-54                [2, 20, 512]              --\n",
       "│    │    └─DecoderBlock: 3-12                          [2, 20, 512]              --\n",
       "│    │    │    └─ModuleList: 4-72                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-168         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-55     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-67          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-169                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-170                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-171                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-172                    [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-173                     [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-72                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-174         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-56                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-175         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-57     [2, 20, 512]              1,024\n",
       "│    │    │    └─MultiHeadAttentionBlock: 4-69          [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-176                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-177                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Linear: 5-178                     [2, 20, 512]              262,144\n",
       "│    │    │    │    └─Dropout: 5-179                    [2, 8, 20, 20]            --\n",
       "│    │    │    │    └─Linear: 5-180                     [2, 20, 512]              262,144\n",
       "│    │    │    └─ModuleList: 4-72                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-181         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-58                [2, 20, 512]              --\n",
       "│    │    │    │    └─ResidualConnection: 5-182         [2, 20, 512]              --\n",
       "│    │    │    │    │    └─LayerNormalization: 6-59     [2, 20, 512]              1,024\n",
       "│    │    │    └─FeedForwardBlock: 4-71                 [2, 20, 512]              --\n",
       "│    │    │    │    └─Linear: 5-183                     [2, 20, 2048]             1,050,624\n",
       "│    │    │    │    └─Dropout: 5-184                    [2, 20, 2048]             --\n",
       "│    │    │    │    └─Linear: 5-185                     [2, 20, 512]              1,049,088\n",
       "│    │    │    └─ModuleList: 4-72                       --                        (recursive)\n",
       "│    │    │    │    └─ResidualConnection: 5-186         --                        (recursive)\n",
       "│    │    │    │    │    └─Dropout: 6-60                [2, 20, 512]              --\n",
       "│    └─LayerNormalization: 2-8                          [2, 20, 512]              1,024\n",
       "├─ProjectionLayer: 1-7                                  [2, 20, 1000]             --\n",
       "│    └─Linear: 2-9                                      [2, 20, 1000]             513,000\n",
       "=========================================================================================================\n",
       "Total params: 45,640,680\n",
       "Trainable params: 45,640,680\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 91.25\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 27.52\n",
       "Params size (MB): 182.56\n",
       "Estimated Total Size (MB): 210.08\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example parameters\n",
    "src_vocab_size = 1000  # 示例的源语言词汇表大小\n",
    "tgt_vocab_size = 1000  # 示例的目标语言词汇表大小\n",
    "src_seq_len = 20  # 示例的源语言序列长度\n",
    "tgt_seq_len = 20  # 示例的目标语言序列长度\n",
    "d_model = 512  # Embedding dimension\n",
    "N = 6  # Number of encoder/decoder layers\n",
    "h = 8  # Number of attention heads\n",
    "dropout = 0.1  # Dropout rate\n",
    "d_ff = 2048  # Feedforward dimension\n",
    "\n",
    "# Create the transformer\n",
    "transformer = build_transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, d_model, N, h, dropout, d_ff)\n",
    "# print(transformer)\n",
    "\n",
    "# Sample input data (dummy data)\n",
    "batch_size = 2\n",
    "src_seq_len = 20\n",
    "tgt_seq_len = 20\n",
    "\n",
    "# Example input tensors (replace with your actual data if needed)\n",
    "src = torch.randint(0, src_vocab_size, (batch_size, src_seq_len))\n",
    "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_len))\n",
    "src_mask = torch.ones(batch_size, src_seq_len)  # Mask for the source sequence\n",
    "tgt_mask = torch.ones(batch_size, tgt_seq_len)  # Mask for the target sequence\n",
    "\n",
    "src_mask = src_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, seq_len, seq_len]\n",
    "tgt_mask = tgt_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "# output model info\n",
    "from torchinfo import summary\n",
    "summary(transformer, input_data=[src, tgt, src_mask, tgt_mask], depth=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
