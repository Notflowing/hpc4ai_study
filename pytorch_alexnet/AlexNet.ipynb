{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional  as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define relevant variables for the Machine Learning task\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.005\n",
    "num_epochs = 30\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "torch.Size([3, 224, 224])\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def get_train_valid_loader(data_dir,\n",
    "                           batch_size,\n",
    "                           random_seed,\n",
    "                           valid_size=0.1,\n",
    "                           shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    valid_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=train_transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=valid_transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    " \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "    \n",
    "    # MNIST dataset samples\n",
    "    class_names = train_dataset.classes\n",
    "    print(train_dataset)\n",
    "    print(valid_dataset)\n",
    "\n",
    "    print(train_dataset[0][0].shape)\n",
    "    print(class_names)\n",
    "    # torch.manual_seed(0)\n",
    "\n",
    "    # fig = plt.figure(figsize=(16, 4))\n",
    "    # rows, cols = 2, 10\n",
    "\n",
    "    # for i in range(1, (rows*cols) + 1):\n",
    "    #     rand_ind = torch.randint(0, len(train_dataset), size=[1]).item()\n",
    "    #     img, label = train_dataset[rand_ind]\n",
    "    #     img = img.numpy()\n",
    "    #     # print(img.numpy().shape) # [3, 224, 224]\n",
    "    #     img = np.transpose(img, (1, 2, 0)) # 把channel那一维放到最后\n",
    "    #     # img = img * [0.485, 0.456, 0.406] + [0.229, 0.224, 0.225]   # unnormalize\n",
    "    #     fig.add_subplot(rows, cols, i)\n",
    "    #     plt.imshow(img)\n",
    "    #     plt.title(f\"{class_names[label]}\")\n",
    "    #     plt.axis(False)\n",
    "    #     plt.tight_layout()\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "def get_test_loader(data_dir,\n",
    "                    batch_size,\n",
    "                    shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "\n",
    "    # define transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=False,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "# CIFAR10 dataset \n",
    "train_loader, valid_loader = get_train_valid_loader(data_dir='./data', batch_size=batch_size, random_seed=1)\n",
    "\n",
    "test_loader = get_test_loader(data_dir='./data', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deep convolutional neural network AlexNet\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10, dropout: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # 这里，我们使用一个11*11的更大窗口来捕捉对象。\n",
    "            # 同时，步幅为4，以减少输出的高度和宽度。\n",
    "            # 另外，输出通道的数目远大于LeNet\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "            # 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "            # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            # 最后是输出层\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "AlexNet (AlexNet)                        [1, 3, 224, 224]     [1, 10]              --                   True\n",
       "├─Sequential (features)                  [1, 3, 224, 224]     [1, 256, 6, 6]       --                   True\n",
       "│    └─Conv2d (0)                        [1, 3, 224, 224]     [1, 96, 55, 55]      34,944               True\n",
       "│    └─ReLU (1)                          [1, 96, 55, 55]      [1, 96, 55, 55]      --                   --\n",
       "│    └─MaxPool2d (2)                     [1, 96, 55, 55]      [1, 96, 27, 27]      --                   --\n",
       "│    └─Conv2d (3)                        [1, 96, 27, 27]      [1, 256, 27, 27]     614,656              True\n",
       "│    └─ReLU (4)                          [1, 256, 27, 27]     [1, 256, 27, 27]     --                   --\n",
       "│    └─MaxPool2d (5)                     [1, 256, 27, 27]     [1, 256, 13, 13]     --                   --\n",
       "│    └─Conv2d (6)                        [1, 256, 13, 13]     [1, 384, 13, 13]     885,120              True\n",
       "│    └─ReLU (7)                          [1, 384, 13, 13]     [1, 384, 13, 13]     --                   --\n",
       "│    └─Conv2d (8)                        [1, 384, 13, 13]     [1, 384, 13, 13]     1,327,488            True\n",
       "│    └─ReLU (9)                          [1, 384, 13, 13]     [1, 384, 13, 13]     --                   --\n",
       "│    └─Conv2d (10)                       [1, 384, 13, 13]     [1, 256, 13, 13]     884,992              True\n",
       "│    └─ReLU (11)                         [1, 256, 13, 13]     [1, 256, 13, 13]     --                   --\n",
       "│    └─MaxPool2d (12)                    [1, 256, 13, 13]     [1, 256, 6, 6]       --                   --\n",
       "├─Sequential (classifier)                [1, 9216]            [1, 10]              --                   True\n",
       "│    └─Dropout (0)                       [1, 9216]            [1, 9216]            --                   --\n",
       "│    └─Linear (1)                        [1, 9216]            [1, 4096]            37,752,832           True\n",
       "│    └─ReLU (2)                          [1, 4096]            [1, 4096]            --                   --\n",
       "│    └─Dropout (3)                       [1, 4096]            [1, 4096]            --                   --\n",
       "│    └─Linear (4)                        [1, 4096]            [1, 4096]            16,781,312           True\n",
       "│    └─ReLU (5)                          [1, 4096]            [1, 4096]            --                   --\n",
       "│    └─Linear (6)                        [1, 4096]            [1, 10]              40,970               True\n",
       "========================================================================================================================\n",
       "Total params: 58,322,314\n",
       "Trainable params: 58,322,314\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.13\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 5.27\n",
       "Params size (MB): 233.29\n",
       "Estimated Total Size (MB): 239.16\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AlexNet()\n",
    "# print(model)\n",
    "summary(model=model, input_size=(1, 3, 224, 224), col_width=20, \n",
    "        col_names=['input_size', 'output_size', 'num_params', 'trainable'], \n",
    "        row_settings=['var_names'], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [704/704], Loss: 1.7298\n",
      "Accuracy of the network on the 5000 validation images: 35.24 %\n",
      "Epoch [2/30], Step [704/704], Loss: 0.8920\n",
      "Accuracy of the network on the 5000 validation images: 51.44 %\n",
      "Epoch [3/30], Step [704/704], Loss: 1.3447\n",
      "Accuracy of the network on the 5000 validation images: 62.3 %\n",
      "Epoch [4/30], Step [704/704], Loss: 1.9061\n",
      "Accuracy of the network on the 5000 validation images: 61.86 %\n",
      "Epoch [5/30], Step [704/704], Loss: 0.4215\n",
      "Accuracy of the network on the 5000 validation images: 67.58 %\n",
      "Epoch [6/30], Step [704/704], Loss: 0.1987\n",
      "Accuracy of the network on the 5000 validation images: 71.48 %\n",
      "Epoch [7/30], Step [704/704], Loss: 0.5428\n",
      "Accuracy of the network on the 5000 validation images: 73.1 %\n",
      "Epoch [8/30], Step [704/704], Loss: 0.8499\n",
      "Accuracy of the network on the 5000 validation images: 74.0 %\n",
      "Epoch [9/30], Step [704/704], Loss: 1.0260\n",
      "Accuracy of the network on the 5000 validation images: 77.2 %\n",
      "Epoch [10/30], Step [704/704], Loss: 0.9407\n",
      "Accuracy of the network on the 5000 validation images: 77.5 %\n",
      "Epoch [11/30], Step [704/704], Loss: 0.7563\n",
      "Accuracy of the network on the 5000 validation images: 78.22 %\n",
      "Epoch [12/30], Step [704/704], Loss: 0.3737\n",
      "Accuracy of the network on the 5000 validation images: 79.56 %\n",
      "Epoch [13/30], Step [704/704], Loss: 0.3787\n",
      "Accuracy of the network on the 5000 validation images: 79.76 %\n",
      "Epoch [14/30], Step [704/704], Loss: 0.4934\n",
      "Accuracy of the network on the 5000 validation images: 80.26 %\n",
      "Epoch [15/30], Step [704/704], Loss: 0.0892\n",
      "Accuracy of the network on the 5000 validation images: 80.46 %\n",
      "Epoch [16/30], Step [704/704], Loss: 0.5127\n",
      "Accuracy of the network on the 5000 validation images: 78.52 %\n",
      "Epoch [17/30], Step [704/704], Loss: 0.0646\n",
      "Accuracy of the network on the 5000 validation images: 79.68 %\n",
      "Epoch [18/30], Step [704/704], Loss: 0.3811\n",
      "Accuracy of the network on the 5000 validation images: 78.82 %\n",
      "Epoch [19/30], Step [704/704], Loss: 0.3682\n",
      "Accuracy of the network on the 5000 validation images: 81.34 %\n",
      "Epoch [20/30], Step [704/704], Loss: 0.0639\n",
      "Accuracy of the network on the 5000 validation images: 80.8 %\n",
      "Epoch [21/30], Step [704/704], Loss: 0.1847\n",
      "Accuracy of the network on the 5000 validation images: 81.22 %\n",
      "Epoch [22/30], Step [704/704], Loss: 0.4049\n",
      "Accuracy of the network on the 5000 validation images: 80.76 %\n",
      "Epoch [23/30], Step [704/704], Loss: 0.1972\n",
      "Accuracy of the network on the 5000 validation images: 81.58 %\n",
      "Epoch [24/30], Step [704/704], Loss: 0.5174\n",
      "Accuracy of the network on the 5000 validation images: 80.08 %\n",
      "Epoch [25/30], Step [704/704], Loss: 0.2131\n",
      "Accuracy of the network on the 5000 validation images: 80.16 %\n",
      "Epoch [26/30], Step [704/704], Loss: 0.3795\n",
      "Accuracy of the network on the 5000 validation images: 79.0 %\n",
      "Epoch [27/30], Step [704/704], Loss: 0.3089\n",
      "Accuracy of the network on the 5000 validation images: 82.36 %\n",
      "Epoch [28/30], Step [704/704], Loss: 0.4852\n",
      "Accuracy of the network on the 5000 validation images: 82.72 %\n",
      "Epoch [29/30], Step [704/704], Loss: 0.0909\n",
      "Accuracy of the network on the 5000 validation images: 81.7 %\n",
      "Epoch [30/30], Step [704/704], Loss: 0.3790\n",
      "Accuracy of the network on the 5000 validation images: 83.34 %\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.005, momentum=0.9)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()   # sets the module in training mode\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    model.eval()    # sets the module in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %' \n",
    "              .format(5000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 82.88 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()    # sets the module in evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %' \n",
    "          .format(10000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORTING A MODEL FROM PYTORCH TO ONNX\n",
    "model = model.to('cpu')\n",
    "model.eval()\n",
    "x = torch.randn(size=(1, 3, 224, 224), dtype=torch.float32, requires_grad=True)\n",
    "torch.onnx.export(model,                                        # model being run\n",
    "                  x,                                            # model input (or a tuple for multiple inputs)\n",
    "                  \"AlexNet.onnx\",                               # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,                           # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,                             # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,                     # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],                      # the model's input names\n",
    "                  output_names = ['output'],                    # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},   # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
